{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Systems and Least Squares Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Linear Equations\n",
    "\n",
    "A system of simultaneous linear equations is written $$Ax = b.$$ Typically, $A$ is a given square matrix of order $n$, $b$ is a given column vector of $n$ components, and $x$ is an unknown column vector of $n$ components.\n",
    "\n",
    "For example, the $2\\times2$ system of equations\n",
    "$$\n",
    "2 x_1 + 3 x_2 = 4\n",
    "$$\n",
    "$$\n",
    "5 x_1 + 4 x_2 = 3\n",
    "$$\n",
    "\n",
    "can be written as \n",
    "$$\n",
    "\\begin{bmatrix}2 & 3\\\\\n",
    "5 & 4\n",
    "\\end{bmatrix} \\begin{bmatrix}x_1\\\\\n",
    "x_2\n",
    "\\end{bmatrix} = \\begin{bmatrix}4\\\\\n",
    "3\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "This equation determines the intersection of two lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams[\"font.family\"] = \"serif\"\n",
    "mpl.rcParams[\"font.size\"] = \"12\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "x1 = np.linspace(-4, 2, 100)\n",
    "\n",
    "x2_1 = (4 - 2 * x1)/3\n",
    "x2_2 = (3 - 5 * x1)/4\n",
    "\n",
    "ax.plot(x1, x2_1, 'r', lw=2, label=r\"$2x_1+3x_2=4$\")\n",
    "ax.plot(x1, x2_2, 'b', lw=2, label=r\"$5x_1+4x_2=3$\")\n",
    "\n",
    "ax.plot(-1, 2, 'ko', lw=2)\n",
    "ax.annotate(\"The intersection point of\\nthe two lines is the solution\\nto the equation system\",\n",
    "            xy=(-1, 2), xycoords='data',\n",
    "            xytext=(-120, -75), textcoords='offset points', \n",
    "            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3, rad=-.3\"))\n",
    "\n",
    "ax.set_xlabel(r\"$x_1$\", fontsize=18)\n",
    "ax.set_ylabel(r\"$x_2$\", fontsize=18)\n",
    "ax.legend();\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical solution of linear system\n",
    "\n",
    "We can solve simple linear equations using a Computer Algebra Systems (CAS), such as Mathematica, Maple, MATLAB's symbolic toolbox, etc.\n",
    "\n",
    "There are two notable Computer Algebra Systems (CAS) for Python:\n",
    "\n",
    "* [SymPy](http://sympy.org/en/index.html) - A python module that can be used in any Python program, or in an IPython session, that provides powerful CAS features. \n",
    "* [Sage](http://www.sagemath.org/) - Sage is a full-featured and very powerful CAS enviroment that aims to provide an open source system that competes with Mathematica and Maple. Sage is not a regular Python module, but rather a CAS environment that uses Python as its programming language.\n",
    "\n",
    "We will use SymPy in Jupyter Notebook.\n",
    "\n",
    "To get started using SymPy in a Python program or notebook, import the module `sympy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get nice-looking $\\LaTeX$ formatted output run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sympy.init_printing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now solve the above equation as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([[1.0,2],[3,4]])\n",
    "print(a)\n",
    "A = sympy.Matrix(a)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = sympy.Matrix([[2, 3], [5, 4]])\n",
    "b = sympy.Matrix([4, 3])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = A.solve(b)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use symbols to represent the linear system and then solve it symbolically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a11, a12, a21, a22 = sympy.symbols(\"a11, a12, a21, a22\")\n",
    "b1, b2 = sympy.symbols(\"b1, b2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = sympy.Matrix([[a11, a12], [a21, a22]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = sympy.Matrix([b1, b2])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = A.solve(b)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One could use the `subs` function to substitute the symbols with numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.subs({a11:2, a12:3, a21:5, a22:4, b1:4, b2:3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful feature of the symbolic approach is to generate code in a program languages for relatively simple expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sympy.printing.pycode(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sympy.printing.ccode(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sympy.printing.pycode(x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** Find the solution to these equations as a function of the free parameter `a`.  \n",
    "$$\n",
    "    x_0 + 6 x_1 + 3 x_2 = 3 \n",
    "$$\n",
    "$$\n",
    "   2 x_0 + 4 x_1 + 3 x_2 = 2\n",
    "$$\n",
    "$$\n",
    "   3 x_0 + 3 x_1 + a x_2 = 1\n",
    "$$\n",
    "* Verify the solution by computing $A*x - b$ using Sympy\n",
    "* At what value of `a` does the solution no longer exist (because the formula tries to divide by zero)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sympy.symbols(\"a\")\n",
    "A = sympy.Matrix([[1,6,3],[2,4,3],[3,3,a]])\n",
    "b = sympy.Matrix([3,2,1])\n",
    "x = A.solve(b)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see more examples of `SymPy` in later lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical solution of linear systems\n",
    "\n",
    "More commonly, linear systems are solved numerically, using functions provided by `scipy.linalg` (or `numpy.linalg`, which has more limited functionality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg as la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[2, 3], [5, 4]])\n",
    "b = np.array([4, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la.solve(A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Solve the following linear system with three unknowns\n",
    "$$\n",
    "\\begin{align*}\n",
    "10x_{1}\\text{−}7x_{2}&=7,\\\\\n",
    "\\text{−}3x_{1}+2x_{2}+6x_{3}&=4,\\\\\n",
    "5x_{1}\\text{−}x_{2}+5x_{3}&=6,\n",
    "\\end{align*}$$\n",
    "or in matrix format,\n",
    "$$\\left[\\begin{array}{ccc}\n",
    "10 & -7 & 0\\\\\n",
    "-3 & 2 & 6\\\\\n",
    "5 & -1 & 5\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "x_{1}\\\\\n",
    "x_{2}\\\\\n",
    "x_{3}\n",
    "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "7\\\\\n",
    "4\\\\\n",
    "6\n",
    "\\end{array}\\right].$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[10, -7, 0],[-3, 2, 6],[5, -1, 5]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([7, 4, 6])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la.solve(A, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving linear system vs. inversion of matrix\n",
    "\n",
    "The solution to $Ax = b$ can be expressed as $x = A^{−1}b$, where $A^{−1}$ is the inverse of $A$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(la.inv(A),b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, typically it is **unnecessary and inadvisable** to actually compute $A^{−1}$, because it is more expensive and less accurate.\n",
    "\n",
    "We can illustrate this with a simple example: when solving $7x=21$ using single precision, it is more efficient and more accurate to compute $21\\div 7$:\n",
    "* Note the source of error here is just a difference in representation error, whereas the error introduced from computing the matrix inverse can be much greater, as we will see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use single-precision to demonstrate the effect of accuracy\n",
    "x = np.float32(21) / np.float32(7)\n",
    "print(\"%.8f\" % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, computing $7^{-1}\\times 21$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use single-precision to demonstrate the effect of loss of accuracy\n",
    "x = np.float32(1 / 7.) * np.float32(21.)\n",
    "print(\"result: %.8f\" % x)\n",
    "print(\" error: %.1e\" % (x-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaining the above\n",
    "* Integers that fit into the mantissa are exactly represented in floating-point numbers, so `21` and `7` can be exactly represented.\n",
    "* Since the result `3` is also a whole integer there is no rounding error in the floating-point division `21/7`\n",
    "* However, `1/7` is not exactly representable as a floating point number so there is some rounding error, consistent with the error guaranteed by IEEE arithmetic --- $\\delta < \\epsilon / 7 \\approx \\text{1.2e-8}$\n",
    "* Finally, the rounding error is amplified by multiplying by 21, so the final error should be less than 21*1.2e-8=3.6e-7 --- which it is.\n",
    "* In this example computing with a scalar value we actually still have good relative error in the result --- this will not in general be the case when computing with matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.finfo(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual and Error\n",
    "\n",
    "When using floating-point numbers, the solution is not exact due to rounding and cancellation errors.\n",
    "\n",
    "Given $Ax = b$, let $x_*$ denote the exact solution, and $x$ denote the numerical solution. \n",
    "\n",
    "The *residual* (also known as the forward error) is defined as \n",
    "\n",
    "$$r = b - Ax.$$\n",
    "\n",
    "The *error* (also known as the backward error) is defined as \n",
    "$$e = x - x_*,$$\n",
    "\n",
    "\\[ Both $e$ and $r$ are vectors. \\]\n",
    "\n",
    "*Gaussian elimination with partial pivoting (a standard algorithm for solving linear equations) is **guaranteed** to produce **small residuals**.  However, the **error in the solution can still be large**.*\n",
    "\n",
    "When $A$ is singular, the linear equation does not have a unique solution (see below). When $A$ is nearly singular, the solution may be sensitive to rounding and other errors (including noise in your data), and hence the error may be large.\n",
    "\n",
    "In the next couple of sections we will use [Hilbert matrices](https://en.wikipedia.org/wiki/Hilbert_matrix) to explore this issue.  These deceptively simple matrices\n",
    "\n",
    "$$H_{ij} = \\frac{1}{i+j+1} \\text{    } i,j=0,1,...$$\n",
    "\n",
    "become increasingly close to being singular as they get larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(la.hilbert(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 12\n",
    "\n",
    "err = np.zeros(N)\n",
    "res = np.zeros(N)\n",
    "\n",
    "# compute errors and residual for a series of test matrices\n",
    "for n in range(5, N):\n",
    "    A = la.hilbert(n)\n",
    "    b = np.dot(A, np.ones(n))\n",
    "\n",
    "    x = la.solve(A, b)\n",
    "    \n",
    "    err[n] = np.max(np.abs(x - np.ones(n)))\n",
    "    res[n] = np.max(np.abs(b - np.dot(A, x)))\n",
    "    \n",
    "    print('For n={0:2d}, the max-error is {1:e} and the max-residual is {2:e}'.format(n, err[n], res[n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also illustrates that you can have a tiny residual (i.e., compute an $x$ such that $|Ax-b|$ is tiny), while having an apparently awful error (i.e., $|x-x_*|$ is large).  \n",
    "\n",
    "Basically what is happening is that there are one more vectors that we will call $x_0$ for which \n",
    "\n",
    "$$ A x_0 \\approx 0$$\n",
    "\n",
    "within the precision of numerical computation (or relative to the size of the noise in your data).  This is called *rank deficiency*. \n",
    "\n",
    "Therefore, if $Ax=b$ we can add any multiple ($c$) of $x_0$ onto $x$ and still satisfy the equation because\n",
    "\n",
    "$ A (x + c x_0) = b \\implies A x + c A x_0  = b  \\implies   A x \\approx b$\n",
    "\n",
    "Since the value of $c$ is not (well) determined by the equation you end up with a result ($x$) that is essentially defined by the noise in your calculation\n",
    "* I.e., the solution is not well defined.\n",
    "\n",
    "In such a situation you need to use a different equation solver that makes the solution well-defined.  We will look below at one such approach for rank deficient or nearly rank deficient equations that is called least-square solution --- i.e., find the shortest (or smallest norm) vector $x$ that solves the equation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norm and Condition Number\n",
    "\n",
    "For vectors, the *norm* is the \"length\" of the vector. The Euclidean norm (or 2-norm) is exactly the Euclidean length of a vector:\n",
    "$$\\Vert x \\Vert \\equiv \\sqrt{\\sum_i x_i^2}.$$\n",
    "\n",
    "It is equal to the Euclidean distance from the origin to the point $x$ in $\\mathbb{R}^n$.\n",
    "\n",
    "<img src=\"https://mathsimulationtechnology.files.wordpress.com/2012/02/la_r2vector_length.jpg\" alt=\"Euclidean distance\" height=\"300\" width=\"300\">\n",
    "\n",
    "More generally, the $p$-norm for $1\\leq p \\leq \\infty$ of a column vector is defined as \n",
    "$$\\Vert x \\Vert_p \\equiv \\sqrt[p]{\\sum_i \\vert x_i\\vert ^p}.$$\n",
    "\n",
    "When $p=\\infty$, the infinity-norm is $\\Vert x \\Vert_\\infty = \\max\\{\\vert x_i\\vert\\}.$\n",
    "\n",
    "<!--img src=\"https://image.slidesharecdn.com/cs445linearalgebraandmatlabtutorial-150831010550-lva1-app6891/95/linear-algebra-and-matlab-tutorial-16-638.jpg\" alt=\"Unit circles in different norms\"-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sensitivity of a matrix is measured by its [*condition number*](https://en.wikipedia.org/wiki/Condition_number). Intuitively, the condition number in $p$ norm is equal to the ratio of the *maximum magnification factor* of $A$ versus the *minimum magnification factor* of $A$ in $p$-norm, i.e.,\n",
    "$$\\kappa_p(A) = \n",
    "\\frac{\\max_{x\\not= 0}\\Vert A x\\Vert_p / \\Vert x\\Vert_p}{\\min_{x\\not= 0}\\Vert A x\\Vert_p / \\Vert x\\Vert_p}.$$\n",
    "\n",
    "<!--img src=\"http://mechanicaldesign.asmedigitalcollection.asme.org/data/journals/jmdedb/929508/md_136_03_031001_f001.png\" alt=\"Stretching factor\" width=\"500\"-->\n",
    "\n",
    "A system is *well-conditioned* if the condition number is close to 1, and it is *ill-conditioned* if the condition number is large (relative to the reciprocal of the square root of machine epsilon).\n",
    "\n",
    "For linear systems, $\\kappa_p(A)\\Vert r\\Vert_p$ is an upper bound of the $\\Vert e\\Vert_p$. It is in general a good qualitative estimation of $\\Vert e\\Vert_p$.\n",
    "\n",
    "Below we use again the Hilbert matrices to illustrate the problem.  For small Hilbert matrices the condition number is not too big and the results look fine.  But for the larger Hilbert matrices the condition number becomes huge and the backward error is big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa = np.zeros(N)\n",
    "\n",
    "# Compute condition numbers of a series of test matrices\n",
    "for n in range(5, N):\n",
    "    kappa[n] = np.linalg.cond(la.hilbert(n), np.inf)\n",
    "\n",
    "ns = np.arange(5,N);\n",
    "plt.semilogy(ns, kappa[ns] * res[ns], 'b-o', label='estimated')\n",
    "plt.semilogy(ns, err[ns], 'r--', label='actual')\n",
    "plt.xlabel('problem size')\n",
    "plt.ylabel('error')\n",
    "plt.legend(loc=2)\n",
    "plt.show()\n",
    "print(kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** For the Sympy matrix example above using $a=1$ and then again with $a=26.999999/8$\n",
    "* Numerically solve the equation\n",
    "* Compute the 2-norm of the forward and backward errors\n",
    "* Explain what you see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficiency\n",
    "\n",
    "For $n$-by-$n$ linear systems, LU factorization requires about $\\frac{2}{3}n^3$ floating-point operations for large $n$.\n",
    "* Thus, its algorithmic complexity is $O(n^3)$\n",
    "\n",
    "This means that if the number of unknowns grow by a factor of 2, then the computational cost will grow by a factor of 8. Therefore, it is impractical to solve very large problems using LU.\n",
    "\n",
    "**It will take about 26 years to solve a dense matrix problem with 1 billion unknowns on the world's fastest computer as of 2016.**\n",
    "\n",
    "Not only can LU factorization be more accurate than computing and applying the inverse matrix, it is also faster since \n",
    "* computing the inverse requires about about 33% more work for large $n$, and\n",
    "* if there are multiple right hand sides (i.e., multiple $b$) then once $L$ and $U$ are computed, the additional solution can be computed in only $O(n^2)$ cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "N = 10\n",
    "ns = np.power(2, np.arange(N)) * 16\n",
    "times = np.zeros(N)\n",
    "\n",
    "# Perform timing use a series of test matrices\n",
    "for i in range(N):\n",
    "    n = ns[i]\n",
    "    A = np.random.random((n,n))\n",
    "    b = np.ones(n)\n",
    "\n",
    "    start = time.time()\n",
    "    x = la.solve(A, b)\n",
    "    end = time.time()\n",
    "    times[i] = end - start \n",
    "    \n",
    "    print('n={0:5d}: it took {1:10g} seconds'.format(n, times[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(ns, times, 'b-o', label='numerical')\n",
    "plt.loglog(ns, 1.e-11 * np.power(ns, 3), 'r--', label='reference')\n",
    "plt.xlabel('problem size')\n",
    "plt.ylabel('seconds')\n",
    "plt.legend(loc=2);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efficiency can be improved by taking advantage of the special structures of the matrix, for example, symmetry ($A=A^T$) and sparsity. See Chapter 10 of Numerical Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overdetermined Systems: Least Squares\n",
    "\n",
    "In practice, we often have linear systems $Ax = b$, where $A$ is rectangular, with $m$ rows and $n$ columns. \n",
    "\n",
    "If $m>n$, the system is typically overdetermined (which is another origin of rank-deficiency).\n",
    "* I.e., we have fewer unknowns than equations defining them so we cannot *exactly* satisfy all of the equations at the same time (unless some of the equations are redundant, a.k.a. linearly dependent).\n",
    "\n",
    "E.g., imagine fitting a straight line $y = v + u x$, which has two parameters $u$ (slope) and $v$ (intercept), to some (probably noisy) data in the form of $m$ points ($(x_0,y_0), (x_1, y_1),\\ldots,(x_{m-1}, y_{m-1})$.  I.e., there are two unknowns and $m$ data points.\n",
    "\n",
    "Simplistically, you want to write these equations\n",
    "$$\n",
    "   v + u x_0   = y_0\\\\\n",
    "$$\n",
    "$$\n",
    "   v + u x_1 = y_1\\\\\n",
    "$$\n",
    "$$\n",
    "   \\vdots \\\\\n",
    "$$\n",
    "$$\n",
    "   v + u x_{m-1} = y_{m-1} \\\\\n",
    "$$\n",
    "which we know we cannot unless the data points indeed exactly fall on a straight line (which they cannot if there is even the tiniest bit of noise). In matrix form these equations become \n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{cc}\n",
    "1 & x_0 \\\\\n",
    "1 & x_1 \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & x_{m-1}\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "v\\\\\n",
    "u\n",
    "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "y_0 \\\\\n",
    "y_1 \\\\\n",
    "\\vdots \\\\\n",
    "y_{m-1}\n",
    "\\end{array}\\right].\n",
    "$$\n",
    "\n",
    "\n",
    "In this case, the problem must be restated restated as find $x$ such that\n",
    "$$Ax \\approx b.$$\n",
    "I.e., instead of seeking an exact solution (because there is not one!) we are seeking a solution that is as close as possible in some sense.\n",
    "\n",
    "This is typically solved by finding a least squares solution, which minimizes the 2-norm of the residual $r = b - Ax$, i.e., minimize \n",
    "$$\\Vert r \\Vert_2^2 = \\sum_{i=1}^{m} r_i ^2 = \\sum_{i=1}^{m} \\left(b_i - \\sum_{j=1}^n a_{ij} x_j \\right) ^2$$\n",
    "\n",
    "In `SciPy`, the method of least squares is provided by `scipy.linalg.lstsq`.   \n",
    "\n",
    "It is often used in fitting noisy data --- in this setting $b$ is your vector of data and $x$ is the vector of fitting parameters.\n",
    "\n",
    "By using in the fit more data points ($m$) than parameters ($n$) you can try to avoid over fitting --- i.e., fitting the noise instead of the signal.\n",
    "\n",
    "### Matrix rank\n",
    "\n",
    "This is an important concept in understanding how \"singular\" is a matrix.  It can be understood as the number of independent columns (or rows) of a matrix --- i.e., the dimension of the vector space in which the matrix operates.  \n",
    "\n",
    "For a square matrix ($A(n,n)$) of dimension $n$, the maximum rank is $n$ (the number of rows or columns).  But effective rank of the matrix is diminished if the matrix is singular --- i.e., if there are one or more vectors ($x$) for which $Ax=0$.\n",
    "\n",
    "For a rectangular matrix ($A(m,n)$), the maximum rank is $\\min (m,n)$. \n",
    "\n",
    "\n",
    "### Singular-value decomposition (SVD)\n",
    "\n",
    "\\[a.k.a. principal-component analysis, factor analysis, ... \\]\n",
    "\n",
    "\\[Knowledge of SVD is beyond the course, but it is so important that you should be aware of it. \\]\n",
    "\n",
    "\n",
    "The [SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition) is the foundation of much of modern computational linear algebra.  It quantifies the concept of matrix rank and enables error analysis of many algorithms.  For instance, immediately above we wrote that a matrix is singular if there are one or more vectors ($x$) for which $Ax=0$.  But in finite-precision arithmetic this statement is not that useful --- how close to zero is bad?  The singular values of a matrix answer this and many other questions.  For instance, the condition number of a matrix is the ratio of its largest and smallest singular values.\n",
    "\n",
    "Some of the Numpy/SciPy algorithms will return the singular values and also might take as input an `rcond` parameter that is use to screen small singular values.  If your results are sensitive to how you set `rcond` (use the default when in doubt) or if the singular values suggest that the condition number of your matrix is greater than $1/\\sqrt{\\epsilon}$ you should pick up a book or chat with an applied mathematician --- they will be happy to see you since we like solving problems and meeting people.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Fitting a polynomial through some data\n",
    "\n",
    "We are given some data ($(x_i, y_i), \\text{  }i=0,1,..m-1$ sampled (with noise) from a function $y(x)$. \n",
    "\n",
    "We want to fit an approximation $\\bar{y}(x)$ through this data --- i.e., solve in a *least-squares sense* the equations\n",
    "$$\n",
    "\\bar{y}(x_i) = y_i, \\text{  }i=0,1,..m-1\n",
    "$$\n",
    "Since when fitting we ideally have many more fitting points ($m$) than parameters ($n$) the matrix equation we end up with is rectangular, and so you cannot just use the `la.solve` function which needs a square matrix.  \n",
    "\n",
    "If our approximation is a quadratic polynomial \n",
    "$$\n",
    "\\bar{y}(x) = c_0 + x c_1 + x^2 c_2\n",
    "$$\n",
    "we want to solve \n",
    "$$\n",
    "c_0 + x_i c_1 + x_i^2 c_2 = y_i, \\text{  }i=0,1,..m-1\n",
    "$$\n",
    "for $c = \\left[\\begin{array}{c}\n",
    "c_{0}\\\\\n",
    "c_{1}\\\\\n",
    "c_{2}\n",
    "\\end{array}\\right]$, again in a least-squares sense.\n",
    "\n",
    "Writing things out a bit more explicitly, the equations are\n",
    "$$\n",
    "c_0 + x_0 c_1 + x_0^2 c_2 = y_0 \\\\\n",
    "$$\n",
    "$$\n",
    "c_0 + x_1 c_1 + x_1^2 c_2 = y_1 \\\\\n",
    "$$\n",
    "$$\n",
    "c_0 + x_2 c_1 + x_2^2 c_2 = y_2 \\\\\n",
    "$$\n",
    "$$\n",
    "\\vdots \\\\\n",
    "$$\n",
    "$$\n",
    "c_0 + x_{m-1} c_1 + x_{m-1}^2 c_2 = y_{m-1}\n",
    "$$\n",
    "which you should be able to see we can rewrite as\n",
    "$$\n",
    "\\left[\\begin{array}{ccc}\n",
    "1 & x_{0} & x_{0}^{2}\\\\\n",
    "\\vdots & \\vdots & \\vdots\\\\\n",
    "1 & x_{m-1} & x_{m-1}^{2}\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "c_{0}\\\\\n",
    "c_{1}\\\\\n",
    "c_{2}\n",
    "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "y_{0}\\\\\n",
    "\\vdots\\\\\n",
    "y_{m-1}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "or, more compactly\n",
    "$$\n",
    "  X c = y\n",
    "$$\n",
    "where $X_{i j} = x_i^j$, $c$ is the vector of coefficients we are solving for, and $y$ is the input data.\n",
    "\n",
    "So, to compute the fit we need to \n",
    "* load the data $y$ into a vector,\n",
    "* construct the matrix $X$, and\n",
    "* call the right routine to solve the least-squares equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, make some noisy test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "# define true model parameters\n",
    "xexact = np.linspace(-1, 1, 100)\n",
    "c0, c1, c2 = 1, 2, 3\n",
    "yexact = c0 + c1 * xexact + c2 * xexact**2\n",
    "\n",
    "# simulate noisy data points\n",
    "m = 100\n",
    "x = 1 - 2 * np.random.random(m)\n",
    "x = np.sort(x) # for plotting it is convenient to have these in order\n",
    "y = c0 + c1 * x + c2 * x**2 + 2*(np.random.random(m)-0.5)\n",
    "\n",
    "# Let's see what the data looks like\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(x, y, 'go', alpha=0.5, label='Simulated data')\n",
    "ax.plot(xexact, yexact, 'k', lw=2, label='True value $y = 1 + 2x + 3x^2$')\n",
    "ax.set_xlabel(r\"$x$\", fontsize=18)\n",
    "ax.set_ylabel(r\"$y$\", fontsize=18)\n",
    "ax.legend(loc=2);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fit the data to the model using linear least squares --- first find and read the documentation for `np.linalg.lstq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lots of different ways to make X ... here are 2\n",
    "X = np.empty((m,3))\n",
    "for i in range(3):\n",
    "    X[:,i] = x**i\n",
    "#X[:,0] = x**0\n",
    "#X[:,1] = x**1\n",
    "#X[:,2] = x**2\n",
    "#print(X)\n",
    "\n",
    "#X = np.vstack([x**0, x**1, x**2]).T\n",
    "#print(X)\n",
    "\n",
    "c, r, rank, sv = la.lstsq(X, y)\n",
    "print(\"The fitted coefficients are\", c)\n",
    "print(\"rank\", rank)\n",
    "print(\"sv\", sv)\n",
    "print(\"residual\", r)\n",
    "\n",
    "# Compute the fit at the given values of x\n",
    "yfit = c[0] + c[1] * x + c[2] * x**2\n",
    "\n",
    "# Plot the input noisy data, the exact values, and the fit\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(x, y, 'go', alpha=0.5, label='Simulated data')\n",
    "ax.plot(xexact, yexact, 'k', lw=2, label='True value $y = 1 + 2x + 3x^2$')\n",
    "ax.plot(x, yfit, 'b', lw=2, label='Least square fit')\n",
    "ax.set_xlabel(r\"$x$\", fontsize=18)\n",
    "ax.set_ylabel(r\"$y$\", fontsize=18)\n",
    "ax.legend(loc=2);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same concept can be done using higher-degree polynomials. However, higher-degree fittings may suffer from oscillations because the problem is ill-conditioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the data to the model using linear least square: \n",
    "# 1st order polynomial\n",
    "X = np.vstack([x**n for n in range(2)]).T\n",
    "print(\"Xcond\", np.linalg.cond(X))\n",
    "c, r, rank, sv = la.lstsq(X, y)\n",
    "yfit1 = sum([cn * x**n for n, cn in enumerate(c)])\n",
    "\n",
    "# 15th order polynomial\n",
    "X = np.vstack([x**n for n in range(16)]).T\n",
    "print(\"Xcond\", np.linalg.cond(X))\n",
    "c, r, rank, sv = la.lstsq(X, y)\n",
    "print(\"fitted coeffs\", c)\n",
    "yfit15 = sum([cn * x**n for n, cn in enumerate(c)])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(x, y, 'go', alpha=0.5, label='Simulated data')\n",
    "ax.plot(xexact, yexact, 'k', lw=2, label='True value $y = 1 + 2x + 3x^2$')\n",
    "ax.plot(x, yfit1, 'b', lw=2, label='Least square fit [1st order]')\n",
    "ax.plot(x, yfit15, 'm', lw=2, label='Least square fit [15th order]')\n",
    "ax.set_xlabel(r\"$x$\", fontsize=18)\n",
    "ax.set_ylabel(r\"$y$\", fontsize=18)\n",
    "ax.legend(loc=2);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you really want to fit a polynomial:** Numpy provides a routine to handle all of this for you --- but behind the scenes it is doing pretty much just like the above.  Note that it returns the coefficients with the highest power first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.polyfit(x, y, 2)\n",
    "print(\"The fitted coefficients are\", c)  # which are the same as computed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** Below is a function that mimics noisy data coming from an experiment.  Fit the generated data to linear, quadratic and cubic functions, print the norm of corresponding residuals, and plot the fits and the data.  Use time $t$ in the range $[0,10]$.\n",
    "* Generate a equally spaced list of 100 time points in \\[0,10\\]\n",
    "* Compute an array of corresponding function values\n",
    "* Compute the polynomial fits\n",
    "* Evaluate the fits at the list of time points and compute the residuals\n",
    "* Print the fitted coefficients and the norm of the residuals\n",
    "* Plot the data and the fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "def f(t):\n",
    "    return math.exp(-0.1*t)+0.1*(random.random()-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading on linear algebra:\n",
    "* Chapter 5 of Numerical Python book\n",
    "* Chapter 10 of Numerical Python book for sparse matrices\n",
    "\n",
    "Reading on Sympy:\n",
    "* Chapter 3 of Numerical Python book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "Adapted from J.R. Johansson's Scientific Python Lectures available at [http://github.com/jrjohansson/scientific-python-lectures](http://github.com/jrjohansson/scientific-python-lectures)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
