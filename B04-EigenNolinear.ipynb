{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenvalue Problems, Sparse problems, and Nonlinear Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalue problems\n",
    "\n",
    "Besides solving linear systems of equations, another major component of linear algebra is the eigenvalue problem.\n",
    "\n",
    "The eigenvalue problem for a matrix $A$ is:\n",
    "\n",
    "$\\displaystyle A x_k = \\lambda_k x_k$\n",
    "\n",
    "where $\\lambda_k$ is the $k$th eigenvalue and $x_k$ is its corresponding eigenvector. \n",
    "\n",
    "What is the significance/meaning of eigenvectors/values?\n",
    "* Acting with $A$ on an arbitrary vector $x$ in general will\n",
    "  - Rotate the vector and make it point in a different direction\n",
    "  - Scale the length of the vector to make it shorter or longer\n",
    "* Eigenvectors are not rotated --- they are just scaled by a constant (the eigenvalue that can be negative or complex).\n",
    "* Thus, rephrasing a problem in terms of eigenvectors can make it easier to solve and easier to interpret what is going on.\n",
    "* Applications\n",
    "  - Understanding vibrations (from molecules to bridges)\n",
    "  - Interpreting large data sets\n",
    "  - All sorts of engineering and physics\n",
    "\n",
    "For simple 2-by-2 or 3-by-3 systems, eigenvalues can be solved symbolically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy\n",
    "\n",
    "sympy.init_printing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps, delta = sympy.symbols(\"epsilon, delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = sympy.Matrix([[eps, delta], [delta, -eps]])\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H.eigenvals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H.eigenvects()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numerical solution of eigenvalue problems is supported as part of `scipy.linalg`. \n",
    "\n",
    "The computational complexity is $O(n^3)$ for $n$ the size of the (square) matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg as la\n",
    "\n",
    "# or equivalently:\n",
    "# from scipy import linalg as la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate eigenvalues of a matrix, use the function `eigvals`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.random((3, 3))\n",
    "A = A + A.T  # What is this doing?\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = la.eigvals(A)\n",
    "print(evals)\n",
    "np.real(evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the eigenvalues of a non-symmetric real matrix may be complex numbers.\n",
    "\n",
    "For calculating both eigenvalues and eigenvectors, use the function `eig`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals, evecs = la.eig(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvectors corresponding to the $k$-th eigenvalue (stored in `evals[k]`) is the $k$-th *column* in `evecs`, i.e., `evecs[:,k]`. \n",
    "\n",
    "To verify this, let's try mutiplying eigenvectors with the matrix and compare to the product of the eigenvector and the eigenvalue:\n",
    "* $A x - \\lambda x$ should be close to zero "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "\n",
    "#              A * x          - lambda   * x\n",
    "la.norm(np.dot(A, evecs[:,k]) - evals[k] * evecs[:,k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also more specialized eigensolvers, like the `eigh` for Hermitian (the complex equivalent to symmetric) matrices. \n",
    "* But, you must be sure that your matrix really is symmetric/Hermitian otherwise you will get incorrect answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = A + A.T\n",
    "evals, evecs = la.eigh(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A)\n",
    "print(la.eig(A)[0])\n",
    "print(la.eigh(A)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.dot(A, A.T)\n",
    "evals, evecs = la.eigh(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Example Application: Vibrating Spring\n",
    "\n",
    "https://en.wikipedia.org/wiki/String_vibration\n",
    "\n",
    "A string under tension (think of a violin string) vibrates at a natural lowest or fundamental frequency  with harmonics that are integer multiples of the fundamental frequency --- i.e., only certain frequencies (or mixtures thereof) are permitted. \n",
    "\n",
    "Imagine a string of length $L$ stretched along the x-axis (attached at $x=0$ and $x=L$) and vibrating up/down in the y direction.  For small amplitude motions, the equation that describes the vibrational modes is\n",
    "\n",
    "$$\n",
    "   - \\frac{T}{\\rho}  \\frac{d^2 y}{d x^2} = \\omega^2 y\n",
    "$$\n",
    "where $y(x)$ is the displacement, $\\rho$ the linear mass density of the string (mass per unit length), and $T$ the tension.  The vibrational frequencies can be computed analytically to be\n",
    "\n",
    "$$\n",
    "  \\omega_i = (i+1) \\frac{\\pi}{L} \\sqrt{\\frac{T}{\\rho}}\n",
    "$$\n",
    "\n",
    "\n",
    "We don't need to dwell on the details of the diffential equation, but instead immediately imagine chopping the string up into $n$ small elements and approximating the motion of each element (technically using a finite-difference approximation).  This yields the matrix equation\n",
    "$$\n",
    "- \\frac{T}{\\rho h^2}\n",
    "\\left[\\begin{array}{cccccc}\n",
    "-2     &   1    &       &      &      &   \\\\\n",
    " 1     &  -2    &   1   &     &     &   \\\\\n",
    "       &   1    &  -2   &  1   &    &  \\\\\n",
    "       &        & \\ddots&\\ddots&\\ddots&   \\\\\n",
    "       &        &  &  1   &  -2   &   1    \\\\\n",
    "       &        &  &     &   1   &  -2   \\\\\n",
    "\\end{array}\\right]\n",
    "\\left[\\begin{array}{c}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "y_3 \\\\\n",
    "\\vdots \\\\\n",
    "y_{n-1} \\\\\n",
    "y_n\n",
    "\\end{array}\\right]\n",
    "=\n",
    "\\omega^2\n",
    "\\left[\\begin{array}{c}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "y_3 \\\\\n",
    "\\vdots \\\\\n",
    "y_{n-1} \\\\\n",
    "y_n\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "which we can see is an eigenvalue problem with $\\omega^2$ as the eigenvalues, i.e.,\n",
    "$$\n",
    " A y = \\lambda y\n",
    "$$\n",
    "with $\\omega = \\sqrt{\\lambda}$.\n",
    "\n",
    "Below we solve this this problem by\n",
    "* Defining the problem parameters\n",
    "* Constructing the matrix\n",
    "* Solving the eigenvalue problem\n",
    "* Printing out the frequencies\n",
    "* Plotting the corresponding eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "L = 20  # length of the string\n",
    "rho = 1  # density of the string\n",
    "T = 1  # tension of the string\n",
    "\n",
    "n = 100  # number of elements\n",
    "h = L / (n + 1)  # size of each element\n",
    "x = np.linspace(0, L, n + 2)\n",
    "\n",
    "# Set up the matrix\n",
    "A = np.zeros((n, n))\n",
    "for i in range(n):\n",
    "    A[i, i] = -2.0\n",
    "for i in range(n - 1):\n",
    "    A[i, i + 1] = 1.0\n",
    "    A[i + 1, i] = 1.0\n",
    "A *= -T / rho / h**2\n",
    "\n",
    "# print(A)\n",
    "\n",
    "# Compute the eigenvalues and vectors\n",
    "evals, evecs = la.eigh(A)\n",
    "\n",
    "# Sort the eigenvalues/vectors into ascending order\n",
    "idx = evals.argsort()\n",
    "# print(idx)\n",
    "evals = evals[idx]\n",
    "evecs = evecs[:, idx]\n",
    "\n",
    "# Print out the first few frequencies and compare with the expected multiple\n",
    "# of the fundamental (lowest) frequency and analytic formula\n",
    "print(\" i   freq[i]   (i+1)*freq[0]    formula\")\n",
    "print(\"--   -------   -------------    --------\")\n",
    "for i in range(5):\n",
    "    freq = (i + 1) * math.sqrt(T / rho) * math.pi / L\n",
    "    print(\n",
    "        \"%2d    %.3f       %.3f           %.3f\"\n",
    "        % (i, math.sqrt(evals[i]), (i + 1) * math.sqrt(evals[0]), freq)\n",
    "    )\n",
    "\n",
    "# For plotting it looks better if we pad with zeros at the fixed end points at x=0 and x=L\n",
    "evecs_padded = np.zeros((n + 2, n))\n",
    "evecs_padded[1:-1, :] = evecs\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(x, evecs_padded[:, 0], \"-b\", label=\"mode-0\")\n",
    "ax.plot(x, evecs_padded[:, 1], \"-g\", label=\"mode-1\")\n",
    "ax.plot(x, evecs_padded[:, 2], \"-r\", label=\"mode-2\")\n",
    "ax.legend(loc=1)\n",
    "plt.show()\n",
    "\n",
    "# 1.571e-01 100\n",
    "# 1.571e-01 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Example Application: Markov Chain\n",
    "\n",
    "A small child, is either sleeping, eating ice cream, or running.  Every 5 minutes it randomly decides what to do next with no dependence on what it has done previously.  \n",
    "\n",
    "**Question:** If we watch the child over a long time, what is the probability that the child is sleeping, running, or eating ice cream?\n",
    "\n",
    "The child is an example of what is called a finite-state machine --- there's a fixed number of states (in this case just 3) and a simple set of rules to transition between states.\n",
    "\n",
    "It is also an example of a [Markov chain](https://en.wikipedia.org/wiki/Markov_chain) which is a probabalistic process in which the transition probabilities only depend upon the current state.  This is a very important tool in many fields (e.g., modern AI, sociology, chemistry, physics, biology, finance, etc.).\n",
    "\n",
    "To compute an answer we need to assign some probabilities. For instance, if the child is running let there be a probability of \n",
    "* 0.1 that it will sleep\n",
    "* 0.6 that it will keep running\n",
    "* 0.3 that it will eat ice cream\n",
    "\n",
    "This information can be depicted graphically\n",
    "\n",
    "<img src=\"state_diagram_pfkfld.png\" width=\"300\">\n",
    "\n",
    "or encoded in a *transition matrix* $P$, for which the sum of each column is 1 (to conserve probability). For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [\"Sleep\", \"Run\", \"Icecream\"]\n",
    "print(\"The states are:\")\n",
    "for pair in enumerate(states):\n",
    "    print(\"  \", pair)\n",
    "\n",
    "print(\"\\nThe meaning of the transition matrix elements:\")\n",
    "# Possible sequences of events\n",
    "transitionName = [\n",
    "    [\"S<--S\", \"R<--S\", \"I<--S\"],\n",
    "    [\"S<--R\", \"R<--R\", \"I<--R\"],\n",
    "    [\"S<--I\", \"R<--I\", \"I<--I\"],\n",
    "]\n",
    "print(transitionName)\n",
    "\n",
    "print(\"\\nThe transition matrix\")\n",
    "P = np.array([[0.2, 0.1, 0.2], [0.6, 0.6, 0.7], [0.2, 0.3, 0.1]])\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(P,0) # Confirm the sum down each column is 1 as required to conserve probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at 3 different ways to compute the answer.\n",
    "\n",
    "*Approach 1:* Simulation of the dynamical process.  \n",
    "1. Start the child in some state --- does it matter which?\n",
    "2. According to the probabilities given in the transition matrix randomly select a new state.  \n",
    "3. Repeat many times and compute statistics.  \n",
    "\n",
    "For very big Markov chains (e.g., in fitting observations using Bayesian statistics, in modeling biological/physical processes, in financial forecasts) this can be the only viable approach, but for this small problem it is actually the slowest and also least accurate (due to statistical noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "CP = np.cumsum(P, 0)  # cumulative probabilities to assist in sampling random moves\n",
    "print(CP)\n",
    "\n",
    "count = np.zeros(3)  # accumulate count of each state\n",
    "s = 0  # initial state\n",
    "Nmove = 1000000\n",
    "for move in range(Nmove):\n",
    "    count[s] += 1  # accumulate statistics\n",
    "    p = random.random()  # random number in [0,1]\n",
    "    for snew in range(3):  # pick new state using cumulative probability\n",
    "        if CP[snew, s] > p:\n",
    "            s = snew\n",
    "            break\n",
    "\n",
    "count = count / Nmove\n",
    "for i in range(len(count)):\n",
    "    print(states[i], count[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Approach 2:* Repeated application of the transition matrix.\n",
    "\n",
    "With the above choice for the numbering of the states, the initial state that the child is running is captured in the vector $s = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$.  \n",
    "\n",
    "So the probabilities that describe the child's next state are described by the vector of probabilities given by multiplying $P$ on $s$\n",
    "$$P s = \\begin{bmatrix} 0.1 \\\\ 0.6 \\\\ 0.3 \\end{bmatrix}.$$\n",
    "\n",
    "And we can keep repeating this process (multiplying by $P$) to obtain the subsequent probabilities until we reach the steady state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.array([1, 0, 0])  # initial state is eating ice cream\n",
    "for move in range(12):\n",
    "    print(\"%2d\" % move, s)\n",
    "    s = np.dot(P, s)\n",
    "print(\"\")\n",
    "for i in range(len(s)):\n",
    "    print(states[i], s[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Question:* Does it matter what state you start from?  Try starting from either running or sleeping states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Approach 3:* Compute the eigenvectors of the transition matrix.\n",
    "\n",
    "We intuited above that the steady-state probabiity distribution could be obtained by repeatedly multiplying by the transition matrix ($P$).   Let $s$ be an eigen vector of $P$ with eigenvalue $\\lambda$.  I.e.,\n",
    "$$\n",
    "P s = \\lambda s\n",
    "$$\n",
    "If we apply $P$ $n$ times we will get\n",
    "$$\n",
    "P^n s = \\lambda^n s\n",
    "$$\n",
    "But if $\\lambda<1$, we will have $\\lambda^n \\rightarrow 0$ for large $n$.  Similarly, if $\\lambda>1$, we will have $\\lambda^n \\rightarrow \\infty$ for large $n$.  Therefore, if there is a well-defined steady state we must have one eigenvalue equal to one, and all others less than one.\n",
    "\n",
    "So, we can find the steady state by computing the eigenvalues of the transition matrix and looking for the eigenvector corresponding to eigenvalue one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals, evecs = la.eig(P)\n",
    "\n",
    "print(\"evals\", np.real(evals))\n",
    "print(evecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvector corresponding to eigenvalue 1 is the steady state or *stationary probability distribution* after we normalize (scale) it so that the sum is 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = evecs[:, 0]\n",
    "v = v / sum(v)\n",
    "for i in range(len(v)):\n",
    "    print(states[i], v[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have already seen, algorithms operating on matrices can become very expensive very quickly due to the comptational complexity often scaling as $O(n^3)$.  The only way we can solve very large problems are if nearly all of the matrix elements are zero --- i.e., if the matrix is sparse.  It is routine to sove linear equations with literally billions of unknowns, as long as the matrix involved is sparse (meaning just circa $O(N)$ non-zero entries).\n",
    "\n",
    "Scipy has a good support for sparse matrices, with basic linear algebra operations (such as equation solving, eigenvalue calculations, etc).\n",
    "\n",
    "There are many possible strategies for storing sparse matrices in an efficient way. Some of the most common are the so-called coordinate form (COO), list of list (LIL) form,  and compressed-sparse column CSC (and row, CSR). Each format has some advantanges and disadvantages. Most computational algorithms (equation solving, matrix-matrix multiplication, etc) can be efficiently implemented using CSR or CSC formats, but they are not so intuitive and not so easy to initialize. So often a sparse matrix is initially created in COO or LIL format (where we can efficiently add elements to the sparse matrix data), and then converted to CSC or CSR before used in real calculations.\n",
    "\n",
    "For more information about these sparse formats, see e.g. http://en.wikipedia.org/wiki/Sparse_matrix\n",
    "\n",
    "When we create a sparse matrix we have to choose which format it should be stored in. For example, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense matrix\n",
    "M = np.array([[1.0, 0, 0, 0], [0, 3, 0, 0], [0, 1, 1, 0], [1, 0, 0, 1]])\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from dense to sparse\n",
    "A = sp.csr_matrix(M)\n",
    "print(A)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from sparse to dense\n",
    "B = A.todense()\n",
    "print(B)\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More efficient way to create sparse matrices: create an empty matrix and populate with using matrix indexing. This avoids creating a potentially large dense matrix. LIL (list of lists) is an efficient format for building a sparse matrix in this manner (though it is not so efficient for computing with a sparse matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = sp.lil_matrix((4, 4))  # empty 4x4 sparse matrix\n",
    "A[0, 0] = 1\n",
    "A[1, 1] = 3\n",
    "A[2, 2] = A[2, 1] = 1\n",
    "A[3, 3] = A[3, 0] = 1\n",
    "print(A)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting between different sparse matrix formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = sp.csr_matrix(A)\n",
    "print(A)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = sp.csc_matrix(A)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute with sparse matrices like with dense matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(A * A).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.dot(A).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.array([1, 2, 3, 4])[:, np.newaxis]\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse matrix - dense vector multiplication\n",
    "A @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but I find the above construction of a column vector impossible to remember, so instead I use\n",
    "v = np.array([1, 2, 3, 4])\n",
    "A.dot(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same result with dense matrix - dense vector multiplication\n",
    "np.dot(A.todense(), v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using sparse matrices, the linear algebra functionality is provided by `scipy.sparse.linalg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse.linalg as sla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear systems can be solved by using `spsolve` or some iterative methods (such as `gmres`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sla.spsolve(A, v)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = la.solve(A.todense(), v)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, info = sla.gmres(A, v)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigenvalue problems are solved by using `eigs` or `eigsh` (for Hermitian matrix). Typically, only a subset of eigenvalues and eigenvectors are computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals, evecs = sla.eigs(A, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now consider the problem of finding the root for a function of the form $f(x) = 0$.\n",
    "\n",
    "For simple problems, we can solve it algebraically/symbolically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy\n",
    "\n",
    "sympy.init_printing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, a, b, c = sympy.symbols(\"x, a, b, c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a * x**2 + b * x + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sympy.solve(a * x**2 + b * x + c, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sympy.roots(a * x**2 + b * x + c, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sympy.solve(a * sympy.cos(x) - b * sympy.sin(x), x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, symbolic computation is very limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sympy.solve(sympy.sin(x)-x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most problems of practical interests, we need to solve nonlinear equations numerically. This can be done using the `scipy.optimize.fsolve` function. It requires an initial guess: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.fsolve(lambda x: np.sin(x) - x, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a more complicated example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# omega_c = 3.0\n",
    "\n",
    "def f(omega, omega_c):\n",
    "    # a transcendental equation: resonance frequencies of a low-Q SQUID terminated microwave resonator\n",
    "    return np.tan(2 * np.pi * omega) - omega_c / omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "x = np.linspace(1.0e-100, 3, 1000)\n",
    "y = f(x, 3.0)\n",
    "mask = np.where(abs(y) > 50)\n",
    "x[mask] = y[mask] = np.NaN  # get rid of vertical line when the function flip sign\n",
    "ax.plot(x, y, label=\"f(x)\")\n",
    "ax.plot([0, 3], [0, 0], \"k\")\n",
    "ax.legend()\n",
    "ax.set_ylim(-5, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may get different solutions from different starting values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.fsolve(f, 0.0, args=3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.fsolve(f, 0.6, args=3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.fsolve(f, 1.1, args=3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `fsolve` also works for multivariate functions. \n",
    "* Just pass in a NumPy array/vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return [x[1] - x[0] ** 3 - 2 * x[0] ** 2 + 1, x[1] + x[0] ** 2 - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.fsolve(f, np.array([1, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bisection method for univariate functions\n",
    "\n",
    "For univariate functions, a simple (but inefficient and  method for solving nonlinear equations is the bisection method, as illustrated below.\n",
    "\n",
    "We are seeking a solution to $f(x)=0$ and are told that the solution lies in the interval $[a,b]$. \n",
    "1. Evaluate the function at the end points ($a$ and $b$)\n",
    "2. Evaluate the function at the mid-point ($m=(a+b)/2$).  \n",
    "3. If the sign of $f(a)$ is the same as that of $f(m)$ it means the root is in the right half of the interval, so set $a=m$. Otherwise, the root must be in the left half, so set $b=m$.\n",
    "4. Repeat steps 2 and 3 until $b-a$ is sufficiently small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function, desired tolerance and starting interval [a, b]\n",
    "f = lambda x: np.exp(x) - 2\n",
    "tol = 0.1\n",
    "a, b = -2, 2\n",
    "x = np.linspace(-2.1, 2.1, 1000)\n",
    "\n",
    "# graph the function f\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "\n",
    "ax.plot(x, f(x), lw=1.5)\n",
    "ax.axhline(0, ls=\":\", color=\"k\")\n",
    "ax.set_xticks([-2, -1, 0, 1, 2])\n",
    "ax.set_xlabel(r\"$x$\", fontsize=18)\n",
    "ax.set_ylabel(r\"$f(x)$\", fontsize=18)\n",
    "\n",
    "# find the root using the bisection method and visualize\n",
    "# the steps in the method in the graph\n",
    "fa, fb = f(a), f(b)\n",
    "\n",
    "ax.plot(a, fa, \"ko\")\n",
    "ax.plot(b, fb, \"ko\")\n",
    "ax.text(a, fa + 0.5, r\"$a$\", ha=\"center\", fontsize=18)\n",
    "ax.text(b, fb + 0.5, r\"$b$\", ha=\"center\", fontsize=18)\n",
    "\n",
    "n = 1\n",
    "while b - a > tol:\n",
    "    m = a + (b - a) / 2\n",
    "    fm = f(m)\n",
    "\n",
    "    ax.plot(m, fm, \"ko\")\n",
    "    ax.text(m, fm - 0.5, r\"$m_%d$\" % n, ha=\"center\")\n",
    "    n += 1\n",
    "\n",
    "    if np.sign(fa) == np.sign(fm):\n",
    "        a, fa = m, fm\n",
    "    else:\n",
    "        b, fb = m, fm\n",
    "\n",
    "ax.plot(m, fm, \"r*\", markersize=10)\n",
    "ax.annotate(\n",
    "    \"Root approximately at %.3f\" % m,\n",
    "    fontsize=14,\n",
    "    family=\"serif\",\n",
    "    xy=(a, fm),\n",
    "    xycoords=\"data\",\n",
    "    xytext=(-150, +50),\n",
    "    textcoords=\"offset points\",\n",
    "    arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3, rad=-.5\"),\n",
    ")\n",
    "\n",
    "ax.set_title(\"Bisection method in action\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton's Method\n",
    "\n",
    "A more sophisticated method is Newton's method, in which\n",
    "$$x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}.$$\n",
    "\n",
    "<img src=\"newtons.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy\n",
    "\n",
    "# define a function, desired tolerance and starting point xk\n",
    "tol = 0.01\n",
    "xk = 2\n",
    "\n",
    "s_x = sympy.symbols(\"x\")\n",
    "s_f = sympy.exp(s_x) - 2\n",
    "\n",
    "f = lambda x: sympy.lambdify(s_x, s_f, \"numpy\")(x)\n",
    "fp = lambda x: sympy.lambdify(s_x, sympy.diff(s_f, s_x), \"numpy\")(x)\n",
    "\n",
    "x = np.linspace(-1, 2.1, 1000)\n",
    "\n",
    "# setup a graph for visualizing the root finding steps\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 4))\n",
    "\n",
    "ax.plot(x, f(x))\n",
    "ax.axhline(0, ls=\":\", color=\"k\")\n",
    "\n",
    "# repeat Newton's method until convergence to the desired tolerance has been reached\n",
    "n = 0\n",
    "while f(xk) > tol:\n",
    "    xk_new = xk - f(xk) / fp(xk)\n",
    "\n",
    "    ax.plot([xk, xk], [0, f(xk)], color=\"k\", ls=\":\")\n",
    "    ax.plot(xk, f(xk), \"ko\")\n",
    "    ax.text(xk, -0.5, r\"$x_%d$\" % n, ha=\"center\")\n",
    "    ax.plot([xk, xk_new], [f(xk), 0], \"k-\")\n",
    "\n",
    "    xk = xk_new\n",
    "    n += 1\n",
    "\n",
    "ax.plot(xk, f(xk), \"r*\", markersize=15)\n",
    "ax.annotate(\n",
    "    \"Root approximately at %.3f\" % xk,\n",
    "    fontsize=14,\n",
    "    family=\"serif\",\n",
    "    xy=(xk, f(xk)),\n",
    "    xycoords=\"data\",\n",
    "    xytext=(-150, +50),\n",
    "    textcoords=\"offset points\",\n",
    "    arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3, rad=-.5\"),\n",
    ")\n",
    "\n",
    "ax.set_title(\"Newton's method in action\")\n",
    "ax.set_xticks([-1, 0, 1, 2])\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most practical methods are some variate of Newton's method or quasi-Newton methods, such as the secant method.\n",
    "\n",
    "The secant method can be interpreted as using a finite difference approximation to estimate the derivative in Newton's method. Similar to the bisection method, the secant method uses two data points to determine a third one at each step. However, it uses not only the signs but also the values to draw a secant line through two points.\n",
    "\n",
    "<img src=secant.png width=300>\n",
    "\n",
    "The slope of the secant is given by\n",
    "$$s_k = \\frac{f(x_k)-f(x_{k-1})}{x_k-x_{k-1}}.$$\n",
    "The intersection of the secant with the $x$ axis is then\n",
    "$$x_{k+1} = x_k - \\frac{f(x_k)}{s_k} = x_k + \\frac{x_k-x_{k-1}}{f(x_{k-1}) / f(x_k) - 1}.$$\n",
    "\n",
    "We will leave this as an exercise in the homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Chapter 5 of Numerical Python book for eigenvalue problem and nonlinear equations\n",
    "* Chapter 10 of Numerical Python book for sparse matrices\n",
    "* For more on Markov chains, see e.g. \n",
    "https://www.datacamp.com/community/tutorials/markov-chains-python-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "Adapted from J.R. Johansson's Scientific Python Lectures available at [http://github.com/jrjohansson/scientific-python-lectures](http://github.com/jrjohansson/scientific-python-lectures)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
