{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Statistical Modeling: Linear Regression\n",
    "\n",
    "**Overview**\n",
    "- Concept of statistical modeling\n",
    "- Defining statistical models with Patsy\n",
    "- Linear regression\n",
    "- Discrete regression: Logistic regression and Poisson model\n",
    "\n",
    "We will use the [statsmodels](https://www.statsmodels.org/stable/index.html) libray which provides classes and functions for defining statistical models and fitting them to observed data, for calculating descriptive statistics and carrying out statistical tests. The api modules collect the publically accessible symbols that the library provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.graphics.api as smg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Patsy](https://patsy.readthedocs.io/en/latest/) library allows us to write statistical models as simple formulas. It is inspired by statiscal software such as R and S.   The statmodels library internally uses the Patsy library and thus we don't need to access the Patsy's functions directly. But we will use Patsy for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main Problem:** For a set of response(dependent) variables $y$, and explanatory(independent) variable $x$, we want to find a relationship (model) between $y$ and $x$:\n",
    "- mathematical model:         $~~~ y = f(x)$\n",
    "- statistical model:        $~~~ y = f(x) + \\epsilon~~$ where $\\epsilon$ is a random variable. A model is statistical when the data ${y_i, x_i}$ has an element of uncertainty (e.g. due to measurement noise) which is described as $\\epsilon$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A widely used model is \n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x + \\epsilon,\n",
    "$$\n",
    "\n",
    "where $\\beta_0$ and $\\beta_1$ are model parameters and $\\epsilon$ is normally distributed with $0$ mean and variance $\\sigma^2$. \n",
    "* If $x$ is a scalar, the model is known as *simple linear regression*.\n",
    "* If $x$ is a vector, the model is known as *multiple linear regression*.\n",
    "* If $y$ is a vector, the model is known as *multivariate linear regression*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we form the model, we construct the so-called design matrices $y$ and $X$ such that the regression problem can be written in matrix form:\n",
    "\n",
    "$$\n",
    "  y = X\\beta + \\epsilon,\n",
    "$$\n",
    "\n",
    "where $y$ is the vector(or matrix) of observations, $\\beta$ is a vector of coefficients and $\\epsilon$ is the residual(error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "\n",
    "Suppose the observed values are\n",
    "$$y = [1,2,3,4,5]$$\n",
    "and there are two independent variables with values\n",
    "$$x_1 = [6,7,8,9,10]$$\n",
    "and\n",
    "$$ x_2 = [11,12,13,14,15]. $$\n",
    "\n",
    "Assume we use the linear model\n",
    "$$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 $$\n",
    "(Note: *linear* with respect to the $\\beta$ coefficients.)\n",
    "\n",
    "Therefore, the design matrix is\n",
    "$$ X = [1, x_1, x_2, x_1 x_2]. $$\n",
    "\n",
    "Here is Python/NumPy to implement this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1,2,3,4,5])\n",
    "x1 = np.array([6,7,8,9,10])\n",
    "x2 = np.array([11,12,13,14,15])\n",
    "X = np.vstack([np.ones(5),x1,x2,x1*x2]).T\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $X$ and $y$, we can solve for $\\beta$ using least-squares method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta, res, rank, svals = np.linalg.lstsq(X,y,rcond=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, constructing the design matrix $X$ was fairly simple. However, it can be more difficult for more complicated models. \n",
    "\n",
    "The `Patsy` library provides a simple [formula language](https://patsy.readthedocs.io/en/latest/formulas.html#the-formula-language)  to handle this.\n",
    "\n",
    "First, we create a dictionary that maps the variable names in the model to the corresponding data arrays:\n",
    "\n",
    "\\[This is very similar to how in Sympy we associated symbol names with Python variables.\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"y\":y, \"x1\":x1, \"x2\":x2}\n",
    "print(data)\n",
    "print(data[\"x2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the model  \n",
    "$$~ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2$$ \n",
    "with Patsy, we can use the formula `y ~ 1 + x1 + x2 + x1*x2` \n",
    "* note we leave out the coefficients\n",
    "\n",
    "From the formula we can easily get the design matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patsy\n",
    "# y, X = patsy.dmatrices(\"y ~ 1 + x1 + x2 + x1*x2\")\n",
    "y, X = patsy.dmatrices(\"y ~ x1*x2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at y and compare with input\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at X and compare with manual construction\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the ordinary linear regression (OLS) class in the `statsmodels` library (instead of `np.linalg.lstsq`) to solve for the parameter vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, X)\n",
    "result = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare this with the answer from np.linalg.lstsq\n",
    "result.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can skip the step of creating the design matrices by using the statmodels formula API (we imported it as `smf`) by the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols(\"y ~ x1*x2\", data)\n",
    "result = model.fit()\n",
    "result.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This saves us time when we want to add and remove terms in the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Instead of using a Python dictionary, put `y`, `x1` and `x2` into a Pandas data frame, and solve using the `statsmodels` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1,2,3,4,5])\n",
    "data = {\"y\":y, \"x1\":x1, \"x2\":x2}\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#look at C01 or the Pandas doc for how to make a data frame from a dictionary with data\n",
    "df = pd.DataFrame(data)\n",
    "model = smf.ols(\"y ~ x1*x2\", df)\n",
    "result = model.fit()\n",
    "result.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Simplified) summary of the Patsy formula syntax\n",
    "\n",
    "|Syntax|Example| Description |\n",
    "|:-|:- |:---|\n",
    "|lhs ~ rhs|y ~ x <br>(equivalent to y ~ 1+x) |~ is used to separate LHS (dependent variables) and <br> RHS (independent variables) |\n",
    "|var * var| x1 * x2 <br>(equivalent to 1+x1+x2+x1*x2) |An interaction term that implicitly contains all lower-order terms|\n",
    "|var + var| x1 + x2 <br>(equivalent to y ~ 1+x1+x2) |+ denotes the union of terms |\n",
    "|var - var| x1 - x2 <br> |- removes the following term |\n",
    "|var:var| x1:x2 |: denotes a pure interaction term (e.g. $x_1\\cdot x_2$)|\n",
    "\n",
    "For a complete syntax, see the Patsy [documentation](https://patsy.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat above to show we can just specify x1*x2 (look at the \n",
    "# design matrices as well as at the solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols(\"y ~ x1*x2\", data)\n",
    "result = model.fit()\n",
    "result.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "**Basic workflow for analyzing a statistic model using statsmodels**:\n",
    "1. Create an instance of model class, for example, using `mod = sm.MODEL(y,X)` or `mod = smf.model(formula, data)` where `MODEL` and `model` are the names of a particular model (e.g. OLS, GLS, Logit, etc)\n",
    "2. Fit the model to the data:  `result = model.fit()`\n",
    "3. Print summary statistics for the result:  `result.summary()`\n",
    "4. Post-process the model fit results by methods and attributes `params`, `resid`, `fittedvalues`, `predict`\n",
    "5. Visualize the result by Matplotlib or `statsmodels.graphics` module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example (linear regression):\n",
    "Consider fitting a model to generated data whose true value is $ y = 1 + 2x_1 + 3x_2 + 4x_1 x_2$.\n",
    "\n",
    "* Sample 100 random data points in \\[-2,2\\] for our independent variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000  # We will repeat with N=1000 and 10000\n",
    "np.random.seed(12345) # so we all get the same values\n",
    "x1 = 4*(np.random.random(N)-0.5)\n",
    "x2 = 4*(np.random.random(N)-0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "* Define a function to compute y_true,\n",
    "* insert corresponding column of values into the data frame, then\n",
    "* examine the frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_true(x1,x2):\n",
    "    return 1 + 2*x1 + 3*x2 + 4*x1*x2\n",
    "\n",
    "y = y_true(x1,x2)\n",
    "df = pd.DataFrame({\"y\":y, \"x1\":x1, \"x2\":x2})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Add normal-distributed noise to the true values and store the result in the \"y\" column of the data frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 0.1*np.random.randn(N)\n",
    "df[\"y\"] += noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1st model:** $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$ \n",
    "\n",
    "**Question:** what is the simplest corresponding Patsy model?  Replace the `????` below with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1: Create an instance of model class (fit the model to the data using ordinary least square)\n",
    "model = smf.ols(\"y ~ x1 + x2\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2: Fit the model to the data\n",
    "result = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step3: Print summary statistics for the result\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What to look for:*\n",
    "- R-squared:  indicates how well the model fits the data. The value is between 0 and 1. The value 1 corresponds to a perfect fit.  \n",
    "- The `coef` column contains the model parameters. \n",
    "- *t-statistics*:   $t$ = coef/(std err).  The greater $|t|$, the more likely that the corresponding coefficient is non-zero (which means that it has a significant predictive power).\n",
    "   - [Recall: the greater $|t|$, the greater the evidence against the null hypothesis. Here the null hypothesis is that the coefficient is $0$.]\n",
    "- p-value:  small p-value (<0.05 ??) indicates that that coefficient is more likely to be non-zero.\n",
    "    - [Recall: small p-value means strong evidence against the null hypothesis.]\n",
    "- 95% range \n",
    "\n",
    "Summary:  \n",
    "* R-squared close to 1 => good fit.\n",
    "* High $t$ or small $p$-value => that coeff is significantly different from $0$.\n",
    "* 95% range of parameter gives sense of how well-defined is the value (or how sensitive is the fit to changes in the value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also get the R-squared directly:\n",
    "result.rsquared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by using ordinary least-square regression we assume that the residuals (of the fitted model and the data) is normally distributed.  Before analyzing data, we might not know if this condition is met. However, we can investigate this by using statistical tests (with null hypothesis that the residuals are normally distributed) and/or plotting the residual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can look at the residual \n",
    "result.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(result.resid)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the normality of the residual\n",
    "z, p = stats.normaltest(result.resid.values)\n",
    "print(p)\n",
    "z, p = stats.shapiro(result.resid.values)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here (with 100 points) the $p$-value is not especially large from the `normaltest` nor especially small from the `shapiro` test, so it is unclear if we should reject the null hypothesis (i.e. the assumption that the residuals are normally-distributed is correct). \n",
    "* Recall that we're considering our first model ($y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$) which does not include the $x_1 . x_2$ term, so we were likely (or at least I was) expecting the error distribution to be skewed and not normally distributed.  \n",
    "* This just underscores that these tests are just guides and cannot be used in isolation.\n",
    "* With more points the tests become more definitive\n",
    "\n",
    "Let's look at the distribution of the residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(result.resid,bins=20); # change to 50 bins with more points\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a graphical method (`qqplot`) to check for normality. The [Q-Q plot](https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot) compares the observed distribution of values between quantiles with that expected theoretically from the normal distribution. If the distribution is normal, you'll get the straight line $y = x$ (i.e., a line through the origin at 45 degrees assuming both axes are on the same scale).\n",
    "\n",
    "Note: I don't really like these plots since how straight is straight enough or how close to 45 degrees?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?smg.qqplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the QQ graphical method to check for normality.\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "smg.qqplot(result.resid, ax=ax, fit=True, line=\"45\") # need fit to get scales of x and y to match\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is some evidence that the first model is not sufficient.  \n",
    "\n",
    "Let's add the interaction term:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2nd model:** $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 $   **.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Repeat the steps from the previous analysis (Steps 1-3) ... again, enter the correct Patsy formula\n",
    "model = smf.ols(\"y ~ x1*x2\", df)\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The r-squared is very close to 1, indicating a nearly perfect fit.\n",
    "\n",
    "Then we look at the residuals and check if the residuals are normally distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(result.resid) # You'll see the residuals are much smaller\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. statisical test\n",
    "z, p = stats.normaltest(result.resid.values)\n",
    "print(p) # we want this to be large\n",
    "z, p = stats.shapiro(result.resid.values)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both p values are now significant --- so we accept the null hypothesis and conclude that the residuals are probably normally distributed\n",
    "\n",
    "Let's look ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(result.resid,bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. qq-plot \n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "smg.qqplot(result.resid, ax=ax, fit=True, line=\"45\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat with 1000 and 10,000 points to see how the two QQ plots differ with more data.\n",
    "\n",
    "If we are happy with the fitted model, we can extract the model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given values of the indepedent variables ($x_1$, and $x_2$ in this case), we can use the `predict` method to get the prediction (the $y$ value).\n",
    "\n",
    "Compute the predictions on a 50x50 mesh between -2 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-2,2,50)\n",
    "X1, X2 = np.meshgrid(x,x)\n",
    "X1 = X1.ravel()\n",
    "X2 = X2.ravel()\n",
    "print(X1)\n",
    "print(X2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata = pd.DataFrame({\"x1\":X1, \"x2\":X2})\n",
    "ynew = result.predict(newdata)\n",
    "ynew.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ynew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ynew = np.array(ynew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the result is a vector\n",
    "ynew.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we must reshape it into a 50x50 grid/mesh/matrix\n",
    "ynew = ynew.reshape(50,50)\n",
    "ynew.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also reshape the X1 and X2 vectors to a square matrix\n",
    "X1 = X1.reshape(50,50)\n",
    "X2 = X2.reshape(50,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the true data and the fitted model\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n",
    "\n",
    "def plot_y_contour(ax, Y, title):\n",
    "    c = ax.contourf(X1, X2, Y, 15, cmap=plt.cm.RdBu)\n",
    "    ax.set_xlabel(r\"$x_1$\", fontsize=20)\n",
    "    ax.set_ylabel(r\"$x_2$\", fontsize=20)\n",
    "    ax.set_title(title)\n",
    "    cb = fig.colorbar(c, ax=ax)\n",
    "    cb.set_label(r\"$y$\", fontsize=20)\n",
    "\n",
    "plot_y_contour(axes[0], y_true(X1, X2), \"true relation\")\n",
    "plot_y_contour(axes[1], ynew, \"fitted model\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets from R\n",
    "The statmodels provides an interface to load data sets to explore.  See http://www.statsmodels.org/dev/datasets/index.html#available-datasets  for available data sets.\n",
    "\n",
    "As an example, we will load a dataset named \"Icecream\" from the package \"Ecdat\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sm.datasets.get_rdataset(\"Icecream\", \"Ecdat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.data.temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this dataset has 4 variables: cons(consumption), income, price, and temp. \n",
    "\n",
    "**Exercise:** Model the consumption as a linear model with price and temperature as independent variables without an intercept/constant term (i.e., forcing the intercept to be zero):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols(\"cons ~ price + temp - 1\", dataset.data)  # again enter the Patsy formula `cons ~ price + temp - 1`\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphical tools like plot_fit (regression plot) can give a quick look at our fitted model\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "smg.plot_fit(result, 0, ax=ax1)\n",
    "smg.plot_fit(result, 1, ax=ax2)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The consumption seems linearly correlated to the temp but doesn't seem so on the price (it's perhaps because the price range is quite small). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## References: \n",
    "- *Numerical Python: A Practical Techniques Approach for Industry*  by Robert Johansson (Chapter 14)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
