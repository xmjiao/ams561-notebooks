{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief introduction to parallel programming with Python\n",
    "\n",
    "You've had brief introduction to computer acrhitecture and should already know that (once you've exhausted algorithmic improvements and using efficient libraries such as NumPy) the main path to increased performance is parallelism --- i.e., exploiting the ability of your computer to do multiple things at once.\n",
    "\n",
    "How many processors/cores/threads does your computer have?\n",
    "* my laptop has 1 processor with 14 physical cores and supports a total of 22 (hyper)threads\n",
    "* yours might have more cores\n",
    "We routinely use both cores when working on the computer --- e.g., doing homework in the word processor while watching Netflix in the browser.\n",
    "\n",
    "At SBU we have the SeaWulf supercomputer\n",
    "* It contains over 165 computers (also called nodes) --- actually now over 450 nodes\n",
    "* Each computer has 2 processors for a total of 28 cores and 56 threads per computer (which has 128 Gbyte memory)\n",
    "* In total there are 4620 cores and 21 Tbyte memory --- actually now over 20K cores\n",
    "\n",
    "In addition to running faster we often need to use parallel computing to use more memory.\n",
    "* There is one computer in SeaWulf that has 72 cores (144 threads) and 3 Tbyte memory.  A fantastic machine for big data analytics.\n",
    "\n",
    "Lots of standard packages can internally use parallelism.\n",
    "\n",
    "Here we explore how to use parallel programming as a general purpose tool using all of the cores within a single computer.\n",
    "\n",
    "There are two related concepts:\n",
    "* concurrency --- two or more actions that can occur in any order, including at the same time.\n",
    "  * `(a+b) + (c+d)` --- yes --- the operations in parenthesis are independent, but note the final (center) addition must be evaluated last.\n",
    "  * `a + b*c` --- no --- you must evaluate the multiplication before the addition\n",
    "* parallelism --- operations actually occur at the same time. This requires concurrency for correctness, and adds simultaneous execution for performance.\n",
    "  * In HPC (high-performance computing), performance is treated with nearly the same level of concern as correctness.\n",
    "\n",
    "Also, \n",
    "* asynchronous execution --- overlapped/interleaved execution of operations, e.g.,\n",
    "  1. start loading a web page\n",
    "  2. switch to another tab and start loading another web page\n",
    "  3. drink coffee while waiting for one of the pages to finish loading\n",
    "  4. read which ever page downloads first\n",
    "* useful when individual operations may take a long time but don't necessarily consume a lot of computer time (e.g., waiting for network services, a human, etc.)\n",
    "\n",
    "\n",
    "To begin, let's pick a problem to solve.\n",
    "\n",
    "Do you remember Tim?   We were interested in computing the probability of him falling into a canal on the way home.\n",
    "\n",
    "A simplified version of the problem is the following random process.  In 1D, Tim is initially located at the origin.  I.e.,\n",
    "$$y(0)=0$$\n",
    "where $y(t)$ is his position as a function of time ($t$) or, equivalently, the number of steps.\n",
    "Each step he takes is chosen from a uniform random distribution between $-1$ and $1$.  I.e.,\n",
    "$$y(t+1) = y(t) + U[-1,1]$$\n",
    "where $U[-1,1]$ denotes the random number.  \n",
    "\n",
    "We want to answer the question \"When (i.e., at what time or after how many steps) does Tim first reach distance $Y$ from the origin, and how does this time depend upon $Y$?\n",
    "* This is called the first-crossing time\n",
    "* It is important in many applications including chemistry, electronics, biology, etc.\n",
    "* It is interesting to also examine the time and/or probability of returning to the origin.\n",
    "* Also interesting is how first-crossing times and return probability change with the number of dimensions.\n",
    "\n",
    "To do this we will write a function to run a single random walk and return the time (number of steps) taken to reach $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def random_walk(Y):\n",
    "    ''' Execute a random walk and return the number of steps to reach +/- Y'''\n",
    "    y = 0.0\n",
    "    n = 0\n",
    "    while abs(y) < Y:\n",
    "        y += (random.random()-0.5)*2.0\n",
    "        n += 1\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(random_walk(10))\n",
    "print(random_walk(10))\n",
    "print(random_walk(10))\n",
    "print(random_walk(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a random number!  Duh.  \n",
    "\n",
    "So instead we need to compute the average (or expectation value) over many walks.\n",
    "\n",
    "Write a function to average the number of steps needed over `Nsample` walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_length(Y,Nsample=300):\n",
    "    mean = 0\n",
    "    for sample in range(Nsample):\n",
    "        mean += random_walk(Y)\n",
    "    return mean/Nsample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run it for Y=10,20,100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_length(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_length(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_length(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow ... as we increase Y it takes longer and longer.  It would be nice to make this calculation faster.\n",
    "\n",
    "Let's write a program that given a list of Ys runs each calculation (task) in parallel and returns the result to us.\n",
    "\n",
    "We will use the Python standard `multiprocessing` module.\n",
    "* https://docs.python.org/dev/library/multiprocessing.html \n",
    "\n",
    "Specifically, we will be using the process pool. Imagine a pool of workers waiting for a manager to give them tasks to execute.  The manager (or main process) \n",
    "* passes data to each worker describe the work that needs doing\n",
    "  * if there is a lot of data the overhead of passing data needs to be offset by the amount of work that needs doing\n",
    "* takes the result as each worker finishes a task and gives more work\n",
    "* once all the work is done sends the workers home (terminates the worker processes)\n",
    "\n",
    "The processes clearly need to know not just some data (in our case the calue of $Y$) but also what code to execute.  Thus they need access to your Python program.\n",
    "\n",
    "[On Windows using slightly older versions of Python, parallelism did not work easily inside Jupyter notebook and some workarounds were necessary]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First a simple example.  We will write a parallel program to compute the sine of each of a list of numbers --- we don't expect to see any speed up since there is so little work to do compared with the overhead of communication between manager and worker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in map(lambda x: x**2, [1,2,3,4,5,6,7]):\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "\n",
    "# For use in a script\n",
    "#if __name__ == \"__main__\":\n",
    "#    mp.freeze_support() # usually only needed for older Pythons on windows\n",
    "# and indent everything below to be protected by the if\n",
    "\n",
    "NUM_WORKERS = 16\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with mp.Pool(processes=NUM_WORKERS) as pool:\n",
    "    results = pool.map(math.sin, [0.1,0.2,0.3,-2.3,77.0])\n",
    "\n",
    "end_time = time.time()   \n",
    "print(results)\n",
    "\n",
    "print(\"Time : %ssecs\" % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python `with` statement\n",
    "* https://effbot.org/zone/python-with-statement.htm\n",
    "* https://www.geeksforgeeks.org/with-statement-in-python/\n",
    "* https://preshing.com/20110920/the-python-with-statement-by-example/\n",
    "* https://docs.python.org/3/reference/compound_stmts.html#the-with-statement\n",
    "* https://realpython.com/python-with-statement/\n",
    "\n",
    "Python exception handling\n",
    "* https://docs.python.org/3/tutorial/errors.html\n",
    "* https://docs.python.org/3/library/exceptions.html\n",
    "* https://docs.python.org/3/reference/compound_stmts.html#the-try-statement\n",
    "* https://docs.python.org/3/reference/simple_stmts.html#the-raise-statement\n",
    "* https://www.programiz.com/python-programming/exception-handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some key ingredients if using a script rather than inline Jupyter notebook\n",
    "* One process (the manager) will be running the script as its main program --- but we don't want all of the worker processes to execute the full script otherwise they too will make pools of workers that in turn will make more workers ... \n",
    "* So we need to use the `if __name__ == \"__main__\":` technique to stop unnecessary code from being executed\n",
    "* For more details https://docs.python.org/dev/library/multiprocessing.html#multiprocessing-programming\n",
    "\n",
    "\n",
    "Also, it *used* to be essential on Windows to have `mp.freeze_support` --- if needed it *must* be the first line after `if __name__ == \"__main__\":`\n",
    "* `NUM_WORKERS` controls the number of processes we will create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's do the real problem.  Instead of executing math.sin(value) we want to execute mean_length(Y)\n",
    "* modify the program to do this\n",
    "* run it\n",
    "* modify the number of workers --- how does the execution time vary?\n",
    "\n",
    "Our list of Y values will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ylist = [10,20,30,40,50,60,70,80,90,100,110,120,130,140,150]\n",
    "print(Ylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with mp.Pool(processes=NUM_WORKERS) as pool:\n",
    "    results = pool.map(mean_length, Ylist)\n",
    "\n",
    "end_time = time.time()   \n",
    "print(results)\n",
    "\n",
    "print(\"Time : %ssecs\" % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This is what I get on my laptop (in a windows virtual machine with 2 cores)\n",
    "\n",
    "* 1 worker  --- 30.0s\n",
    "* 2 workers --- 18.5s\n",
    "* 4 workers --- 16.6s\n",
    "\n",
    "On my laptop under Linux\n",
    "* 1 worker  --- 23.7s\n",
    "* 2 workers --- 14.4s\n",
    "* 4 workers --- 14.4s\n",
    "\n",
    "On SeaWulf login node\n",
    "* 1 worker  --- 32.0s\n",
    "* 2 workers --- 15.7s\n",
    "* 4 workers --- 9.6s\n",
    "* 15 workers --- 5.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results from the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = [313.9866666666667, 1150.01, 2672.4133333333334, 4621.51, 7085.17, 11509.996666666666, 15458.16, 20049.47, 22710.116666666665, 31472.48, 38479.166666666664, 42849.293333333335, 54146.223333333335, 57255.62333333334, 70433.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(Ylist,T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Texact = [3.14159*Y*Y for Y in Ylist]\n",
    "plt.plot(Ylist,T,\"r-\", Ylist,Texact, \"b-\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we make the calculation run even faster?\n",
    "* order of evaluations\n",
    "* chunking\n",
    "* imap\n",
    "* imap_unordered\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the fastest we can run this calculation?\n",
    "\n",
    "Concept of critical path --- the sequence of computational steps that limit execution time.\n",
    "\n",
    "Concept of performance model\n",
    "\n",
    "**Amdahl's law.**  Imagine that it takes you $S$ seconds to get ready for work (pencil sharpening, making coffee, planning, etc.), and that you have lots of independent tasks which need doing that together take $P$ seconds. Your execution time with just one worker (you) is\n",
    "$$ T(1) = S + P $$\n",
    "But if you had $w$ workers your execution time could be\n",
    "$$ T(w) = S +  \\frac{P}{w} $$\n",
    "\n",
    "($P$ for parallel work; $S$ for sequential work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "S = 100\n",
    "P = 10000\n",
    "T = lambda w : S + P/w \n",
    "wlist = np.arange(1,201,1)\n",
    "plt.plot(wlist, np.log10(T(wlist)));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much faster can we make the calculation run?  I.e., if we had an infinite number of processors (I *want* that computer!!) how fast could it run? \n",
    "\n",
    "If we are using $w$ workers the speedup over using 1 worker is given by\n",
    "$$ \\text{speedup} = \\frac{T(1)}{T(w)} = \\frac{S+P}{S+P/w}  $$\n",
    "\n",
    "If we have an infinite number of processors, then $P/w \\rightarrow 0$ and\n",
    "$$ \\text{speedup} = \\frac{S+P}{S} \\approx \\frac{P}{S}$$\n",
    "where we assumed that $P \\gg S$.\n",
    "\n",
    "I.e., the speedup is limited by the amount of sequential work you have.  \n",
    "\n",
    "Amdahl's law is often said to be **cruel**.  If you want a 100-fold speedup, then only 1% of your work can be sequential.  If you want a 1,000,000-fold speedup, then only 0.0001% can be sequential.  Note that the largest computers in the world has $O(10^{18})$ parallelism!!!!!!!!!!\n",
    "\n",
    "In our example above, with $S=100$ and $P=10000$ only a speedup of 100 is possible no matter how hard you try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speedup = lambda w : (S+P)/(S+P/w)\n",
    "plt.plot(wlist, speedup(wlist));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideal speedup is equal to the number of workers (processors) used.\n",
    "\n",
    "The concept of efficiency is also useful.  Efficiency is defined as the ratio between the possible speedup and the ideal speedup.\n",
    "$$\\text{efficiency}(w) = \\frac{\\text{speedup}(w)}{w} = \\frac{S+P}{wS+P}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff = lambda w : (S+P)/(w*S+P)\n",
    "plt.plot(wlist,eff(wlist));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efficiency is not an abstract concept --- it translates into time, money spent for cloud services, energy consumed, $CO_2$ produced, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we make the computation of a single random walk run in parallel?\n",
    "* Not as defined --- it is an inherently sequential process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the multiprocessing module to parallelize this Monte Carlo algorithm to compute $\\pi$.\n",
    "(https://www.geeksforgeeks.org/estimating-value-pi-using-monte-carlo/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "def sample_one_point():\n",
    "    x,y = random(),random()\n",
    "    rsq = x*x + y*y\n",
    "    if rsq <= 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def sample_one_batch(Npoint = 10000):\n",
    "    sum = 0.0\n",
    "    for i in range(Npoint):\n",
    "        sum += sample_one_point()\n",
    "    return 4*sum/Npoint\n",
    "\n",
    "sum = 0.0\n",
    "sumsq = 0.0\n",
    "Nbatch = 100\n",
    "for i in range(Nbatch):\n",
    "    sample = sample_one_batch()\n",
    "    sum += sample\n",
    "    sumsq += sample**2\n",
    "\n",
    "PM = \"\"\n",
    "mean = sum/Nbatch\n",
    "meansq = sumsq/Nbatch\n",
    "err = math.sqrt((meansq - mean**2)/Nbatch)\n",
    "print(\"%.5f\\u00B1%.5f\" % (mean, err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.Pool(processes=NUM_WORKERS) as pool:\n",
    "    results = pool.map(sample_one_batch, [10000]*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def batch(junk):\n",
    "    return sample_one_batch(100000)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "results = []\n",
    "with mp.Pool(processes=NUM_WORKERS) as pool:\n",
    "    for value in pool.imap_unordered(batch, range(1000), chunksize=4):\n",
    "        results.append(value)\n",
    "    \n",
    "print(\"used %.2fs\" % (time.time()-start))\n",
    "#print(results)\n",
    "results = np.array(results)\n",
    "print(\"%.5f\\u00B1%.5f\" % (results.mean(), results.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you speed this up using Numpy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_one_batch(Npoint = 10000):\n",
    "    x = np.random.random(Npoint)\n",
    "    y = np.random.random(Npoint)\n",
    "    rsq = x*x + y*y\n",
    "    return 4*np.sum(rsq<=1)/Npoint\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def batch(junk):\n",
    "    return sample_one_batch(100000)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "results = []\n",
    "with mp.Pool(processes=NUM_WORKERS) as pool:\n",
    "    for value in pool.imap_unordered(batch, range(1000), chunksize=4):\n",
    "        results.append(value)\n",
    "    \n",
    "print(\"used %.2fs\" % (time.time()-start))\n",
    "#print(results)\n",
    "results = np.array(results)\n",
    "print(\"%.5f +/- %.5f\" % (results.mean(), results.std()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got about a 10x speedup switching from Python to NumPy ... what's the moral?  \n",
    "\n",
    "Start close to home for optimization\n",
    "1. Do less work by only computing what you need.\n",
    "1. Use the best algorithm.\n",
    "2. Efficient use of a single process (NumPy, external libraries, etc.) which translates to intra-process use of intra-processor parallelism.\n",
    "3. Multi-thread (intra-process) and/or multi-process parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coming back to Tim.  Can you speedup up simulation of one Tim using NumPy? \n",
    "\n",
    "No.  It is sequential and is not working on a vector of data.\n",
    "\n",
    "But multiple, **independent** Tims?  Yes.  This requires moving the loop over samples inside the loop over steps and having a vector of samples.  This process of loop interchange and promotion of data structures (scalar to vector) is common when optimizing code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comunicating processes\n",
    "\n",
    "This is an advanced topic --- for most purposes try to start with `map` or `imap` or `imap_unordered`.\n",
    "\n",
    "For parallel programming, my rule-of-thumb is that for each factor of 10 speedup via parallelism there is about a factor of 2 increase in the programming cost\n",
    "* Fugaku, the currently fastest computer in the world, has about 150,000 processors\n",
    "* Programs that can use Fugaku efficiently are about 32-64x more expensive to write than programs for just one core (e.g., 1 month of programming versus 2-5 years).  \n",
    "* Is your problem worth that effort?\n",
    "\n",
    "Sometimes processes need to coordinate with each other and exchange data in order to parallelize a calculation.  Examples include:\n",
    "* Client-server: A process is running in the background responding to requests --- e.g., database or web server\n",
    "* Domain decomposition: E.g., computational fluid dynamcs simulation of the atmosphere\n",
    "* Parallel linear algebra: operating on large matrices\n",
    "\n",
    "**Example**: Use `process` and `pipe` from `multiprocessing` to create a server process connected via a pipe. It should forever\n",
    "* receive a string, and \n",
    "* reply with the received value with \"X\" appended (e.g., if you received \"hello\" reply with \"helloX\")\n",
    "* if the value is \"stop\" close the connection and return\n",
    "Test the server by sending some test strings and then \"stop\".  The main process should also join the child process to clean up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "def server(connection):\n",
    "    while True:\n",
    "        value = connection.recv()\n",
    "        connection.send(value + \"X\")\n",
    "        if value == \"stop\":\n",
    "            break\n",
    "    connection.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parent_conn, child_conn = mp.Pipe()\n",
    "    p = mp.Process(target=server, args=(child_conn,))\n",
    "    p.start()\n",
    "    for msg in [\"a\",\"fred\",\"Who am I?\", \"stop\"]:\n",
    "        parent_conn.send(msg)\n",
    "        print(\"sent\", msg, \"received\", parent_conn.recv())\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** Using `process` and `pipe` from `multiprocessing` write a program that \n",
    "* creates a \"server\" process connected to the main program with a pipe\n",
    "* the server process *forever* receives values from the pipe\n",
    "  * if the value is \"hello\" it replies \"yes\"\n",
    "  * if the value is \"bye\" it replies \"no\", closes the pipe, and returns\n",
    "* the main process \n",
    "  * sends \"hello\" 10 times to the server printing out the reply each time\n",
    "  * sends \"bye\", checks that the reply is no, and then joins the process and exits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "def server(connection):\n",
    "    while True:\n",
    "        value = connection.recv()\n",
    "        if value == \"hello\":\n",
    "            connection.send(\"yes\")\n",
    "        elif value == \"bye\":\n",
    "            connection.send(\"no\")\n",
    "            break\n",
    "        else:\n",
    "            connection.close()\n",
    "            raise ValueError(\"Server: Expecting only hello or bye but received: %s\" % str(value))\n",
    "            \n",
    "    connection.close()\n",
    "\n",
    "def customer(connection):\n",
    "    for i in range(10):\n",
    "        connection.send(\"hello\")\n",
    "        value = connection.recv()\n",
    "        print(\"I received\", value)\n",
    "    connection.send(\"bye\")\n",
    "    value = connection.recv()\n",
    "    if value != \"no\":\n",
    "        raise ValueError(\"Customer: Expecting 'no' but recevied %s\" % str(value))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parent_conn, child_conn = mp.Pipe()\n",
    "    p = mp.Process(target=server, args=(child_conn,))\n",
    "    print(p.name)\n",
    "    p.start()\n",
    "    customer(parent_conn) \n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution of partial differential equation in parallel\n",
    "\n",
    "[Don't worry about the calculus --- it will vanish very soon.]\n",
    "\n",
    "We will employ domain decomposition to solve the differential equation (eigen problem)\n",
    "$$\n",
    " \\frac{\\partial^2 }{\\partial x^2} f(x) = - f(x)\n",
    "$$\n",
    "on the domain $[0,\\pi]$ with boundary conditions $f(0) = f(\\pi) = 0$.  \n",
    "* A solution is $f(x) = \\alpha \\sin(x)$ for any value of constant $\\alpha$.\n",
    "\n",
    "We will approximate the second derivative using the three-point stencil\n",
    "$$\n",
    " \\frac{\\partial^2 f(x)}{\\partial x^2} \\approx \\frac{1}{h^2} \\left(f(x+h) + f(x-h) - 2 f(x)\\right)\n",
    "$$\n",
    "where $h$ is the spacing between grid points in the $x$-dimension --- i.e., $x_i = i h$.  \n",
    "* Note that to compute the second derivative at point $x_i$ we need the function values at $x_{i-1}$, $x_i$, and $x_{i+1}$.\n",
    "* We also use the notation $f_i = f(x_i)$\n",
    "\n",
    "Finally, we will update using Euler's method\n",
    "$$\n",
    " f(x) \\leftarrow f(x) + dt \\frac{\\partial^2 }{\\partial x^2} f(x)\n",
    "$$\n",
    "for some small value $dt$ interpreted as a time step (in imaginary time if you are really interested).\n",
    "\n",
    "Thus, to solve the equation we \n",
    "1. Initialize $f(x_i)$ with $x_i = i h$ and ensuring $f(0) = f(\\pi) = 0$.\n",
    "2. Repeat until converged $f(x_i) \\leftarrow f(x_i) + dt \\left(f(x_{i+1}) + f(x_{i-1}) - 2 f(x_i)\\right) / h^2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell defines necessary parameters and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "N = 51              # No. of points used --- need this to be odd\n",
    "h = math.pi/(N-1)   # Element size\n",
    "dt = 0.4*h**2       # Time step (must be less than (1/2)*h**2)\n",
    "nstep = int(5.0/dt) # No. of steps required to converge\n",
    "print(\"    N\", N)\n",
    "print(\"    h %.4f\" % h)\n",
    "print(\"   dt %.4f\" %dt)\n",
    "print(\"nstep\", nstep)\n",
    "\n",
    "# We'll solve for $f(x_i)$ at these points\n",
    "x = np.linspace(0.0,math.pi,N)\n",
    "\n",
    "def guess(x):\n",
    "    return x*(math.pi-x) / (0.25*math.pi**2)\n",
    "\n",
    "def diff2(f,h):\n",
    "    ''' Estimates d2f/dx2 using 3 point stencil '''\n",
    "    N = f.shape[0]\n",
    "    df2 = np.zeros(N)\n",
    "    df2[1:-1] = (f[2:]+f[:-2] - 2*f[1:-1])*h**-2\n",
    "    #for i in range(1,N-1):\n",
    "    #    df2[i] = (f[i+1] + f[i-1] - 2*f[i])/h**2\n",
    "    return df2\n",
    "\n",
    "def euler(f,dt,h):\n",
    "    ''' Advances one time step using f(t+dt) = f(t) + dt * d2f/dx2 '''\n",
    "    return f + dt * diff2(f,h)\n",
    "\n",
    "def doplot(f, x):\n",
    "    fac = math.sin(x[N//4])/f[N//4]\n",
    "    f = fac*f # rescale to make comparison to exact easier \n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.plot(x, f, 'b-', x, np.sin(x), 'k-')\n",
    "    ax1.set_ylabel(\"f(x)\")\n",
    "    ax1.set_xlabel(\"x\")\n",
    "    ax2.plot(x, f-np.sin(x), 'r-')\n",
    "    ax2.set_ylabel(\"error\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First,** solve across the entire domain $[0,\\pi]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = guess(x)\n",
    "for step in range(nstep):\n",
    "    f = euler(f,dt,h)\n",
    "    \n",
    "doplot(f, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second,** solve by dividing the domain into two $[0,\\frac{\\pi}{2}] + [\\frac{\\pi}{2}, \\pi]$\n",
    "\n",
    "The domains will be split between points $x_m = \\frac{\\pi}{2}$ and $x_{m+1}$, with $m=\\frac{N-1}{2}$.  \n",
    "\n",
    "Because of this, the solutions are coupled.  \n",
    "* On the left, to compute the second derivative at point $x_{m}$ we will need the value at $x_{m+1}$\n",
    "* On the right, to compute the second derivative at point $x_{m+1}$, we will need the value at $x_{m}$\n",
    "* These values must be exchanged every time step\n",
    "\n",
    "But now --- the left and right sides can be updated in parallel!\n",
    "* How do we know this?  Below the updates of `fleft` and `fright` can be done in any order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = guess(x)\n",
    "m = N//2\n",
    "fleft = f[0:m+1]\n",
    "fright = f[m-1:]\n",
    "\n",
    "for iter in range(nstep):\n",
    "    fleft = euler(fleft, dt, h)\n",
    "    fright = euler(fright, dt, h)\n",
    "    fleft[-1] = fright[1]\n",
    "    fright[0] = fleft[-2]\n",
    "\n",
    "f = np.concatenate((fleft[:-1],fright[1:]))\n",
    "doplot(f, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Third,** make it run in parallel using just two processes.\n",
    "* We make a new process and number \n",
    "  * The original process as `id=0`\n",
    "  * The new process as `id=1`\n",
    "* Process `0` will simulate the left and process `1` the right\n",
    "* When we start the new process we pass `id,dt,h,fright`\n",
    "* Each process iterates, exchanging data each time step\n",
    "* When finished, process `1` sends the solution `fright` process `0`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(conn, id, dt, h, nstep, f):\n",
    "    for step in range(nstep):\n",
    "        f = euler(f, dt, h)\n",
    "        if id == 0:\n",
    "            f[-1] = conn.recv()\n",
    "            conn.send(f[-2])\n",
    "        else:\n",
    "            conn.send(f[1])\n",
    "            f[0] = conn.recv()\n",
    "\n",
    "    if id == 0:\n",
    "        fright = conn.recv()\n",
    "        return np.concatenate((f[:-1],fright[1:]))\n",
    "    else:\n",
    "        conn.send(f)\n",
    "        conn.close()\n",
    "\n",
    "f = guess(x)\n",
    "m = N//2\n",
    "fleft = f[0:m+1]\n",
    "fright = f[m-1:]\n",
    "\n",
    "parent_conn, child_conn = mp.Pipe()\n",
    "p = mp.Process(target=solve, args=(child_conn, 1, dt, h, nstep, fright))\n",
    "p.start()\n",
    "f = solve(parent_conn, 0, dt, h, nstep, fleft) \n",
    "p.join()\n",
    "\n",
    "doplot(f, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
