{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical computation\n",
    "\n",
    "1. Representation of integers in Python and NumPy\n",
    "   - Maximum/minimum values, overflow\n",
    "1. Concepts of\n",
    "   - Absolute error\n",
    "   - Relative error\n",
    "   - Precision and representation error\n",
    "   - Accuracy\n",
    "2. Representation of floating point numbers in Python and NumPy\n",
    "   - Maximum/minimum values, machine precision, overflow and underflow\n",
    "   - Representation error\n",
    "   - Roundoff error\n",
    "   - Commutative, associative and distributive properties\n",
    "   - Comparison of floating point values\n",
    "   - Cancellation error\n",
    "   - Summation example\n",
    "   - Quadratic equation example\n",
    "   - Extended precision\n",
    "1. Classic very detailed reference about floating point numbers https://www.itu.dk/~sestoft/bachelor/IEEE754_article.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integers\n",
    "\n",
    "### Division operation\n",
    "\n",
    "In Python3, integer division is not closed, unless you use the `//` operator.  I.e., if you use '/' to divide two integers the result is always a floating point number even if the result is whole number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Python3:    4/2\", 4/2)\n",
    "print(\"Python3:    3/2 =\", 3/2, \"    3//2 =\", 3//2, \"   1//2 =\",1//2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NumPy behaves the same way*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(10)\n",
    "print(a.dtype)\n",
    "print(a)\n",
    "print(a/2)\n",
    "print(a/np.full(10,2))\n",
    "print(a//2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allowed values of integers\n",
    "\n",
    "Native Python3 integers can be arbitrarily large (positive or negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "print(\" m       2**m             2**m in binary format\")\n",
    "print(\"--    ----------     -------------------------------------\")\n",
    "\n",
    "for iteration in range(35):\n",
    "    print(\"{0:2d}  {1:12d}  {1:40b}\".format(iteration,i))\n",
    "    i*=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.int32(2**1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, Numpy (and Python2 and nearly every other programming language you will use) has integers with a fixed size for compact storage and efficient computing (since this is what the computer hardware actually uses)\n",
    "\n",
    "E.g., a 32-bit integer is a sequence of 32 bits (4 bytes) using two's complement representation (https://en.wikipedia.org/wiki/Two%27s_complement)\n",
    "\n",
    "```\n",
    " [bit31, bit30, bit29, bit28, ..., bit2, bit1, bit0]\n",
    " ```\n",
    "\n",
    "$i = - b_{31}*2^{31} +  b_{30}*2^{30} +  b_{29}*2^{29}+  b_{28}*2^{28} + \\cdots + b_{2}*2^{2} + b_{1}*2^{1} + b_{0}*2^0 $\n",
    "\n",
    "Most modern computers support in hardware 8-bit, 16-bit, 32-bit and 64-bit integers (with and without signs)\n",
    "\n",
    "The default integer in NumPy is 64-bit.\n",
    "\n",
    "If you are curious, or if you ever need to do it by hand:\n",
    "* Computing the binary representation of a positive number is straightforward.  In decreasing order, subtract powers of two less than the value.  For example, `265`.  The largest power of 2 less than 265 is $2^8=256$.  Subtracting 256 leaves $9$, which you can see is `8+1 = 2^3 + 2^0`, so the 32-bit binary representation of `265` is `00000000000000000000000100001001`.\n",
    "* For a negative integer $-i$ the lower 31 bits are computed from $2^{31}-i$ and the 32nd bit is set.  So armed with the binary representation of +265 we can compute the two's-complement binary representation of -265 by first computing (in binary) $2^{31}-265$.\n",
    "\n",
    "```\n",
    "  10000000000000000000000000000000\n",
    "- 00000000000000000000000010001001\n",
    "= 01111111111111111111111101110111\n",
    "```\n",
    "\n",
    "and then seting the upper bit to obtain `11111111111111111111111011110111`.\n",
    "* Notice that the representation of the negative value just requires \"flipping\" each bit of the positive value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** In two's-complement 32-bit format, what are the binary representations of \n",
    "* 3\n",
    "* -3\n",
    "\n",
    "If you want to check your answer look [here](https://www.exploringbinary.com/twos-complement-converter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aside: Python can print binary representations of integers but does not internally use the\n",
    "# two's-complement representation and so negative numbers appear just with a minus sign\n",
    "print(f\"{3:b}\")\n",
    "print(f\"{-3:b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a function to convert an integer into a string containing its binary 32-bit two's-complement representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b(i):\n",
    "    ''' Returns bit-string representation of a 32-bit two's-complement integer '''\n",
    "    if i<-2**31 or i>=2**31:\n",
    "        raise(ValueError)\n",
    "    s = \"0\"\n",
    "    if i<0:\n",
    "        s = \"1\"\n",
    "        i += 2**31\n",
    "    for q in range(30,-1,-1):\n",
    "        s += [\"0\",\"1\"][(i&(1<<q))>>q]\n",
    "    return s\n",
    "\n",
    "print(-2147483648, b(-2147483648)) # The most negative integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test it on some integers from numpy\n",
    "import numpy as np\n",
    "i = np.array([3],np.int32)\n",
    "print(\"{0:11d} {1}\".format(i[0],b(i[0])))\n",
    "i[0] = -3\n",
    "print(\"{0:11d} {1}\".format(i[0],b(i[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2147483647\n",
    "print(x)\n",
    "x = x + 1\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fixed width of NumPy integers means they cannot be arbitrarily large. The largest positive value you can put in a 32-bit signed integer must fit into 31 bits which is\n",
    "\n",
    "$2^{31}-1$ = 2147483647\n",
    "\n",
    "And the largest magnitude negative value is \n",
    "$-2^{31}$ = -2147483648\n",
    "\n",
    "For the default 64-bit integers the limits are $2^{63}-1$ = 9223372036854775807 and $-2^{63}$=-9223372036854775808\n",
    "\n",
    "**Exercise:** Explain the following behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unexpected things can happen when you \"overflow\" a fixed-size integer\n",
    "\n",
    "i = np.array([2**31-1],np.int32)\n",
    "print(\"{0:11d} {1}\".format(i[0],b(i[0])))\n",
    "i += 1\n",
    "print(\"{0:11d} {1}\".format(i[0],b(i[0])))\n",
    "print()\n",
    "i = np.array([-2**31],np.int32)\n",
    "print(\"{0:11d} {1}\".format(i[0],b(i[0])))\n",
    "i -= 1\n",
    "print(\"{0:11d} {1}\".format(i[0],b(i[0])))\n",
    "print(\"{0:11d} {1}\".format(-1,b(-1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore increasing powers of 2\n",
    "i = np.array([1],np.int32)\n",
    "for iteration in range(40):\n",
    "    print(\"{0:2d}  {1:11d}   {2}\".format(iteration,i[0],b(i[0])))\n",
    "    i *= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:** *these issues only affect very large integers in NumPy (or other programming languages including Python2) but most of the time (since you are usually working with small values) things will behave as you expect.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absolute error, relative error, significant digits, precision, and accuracy\n",
    "\n",
    "These are central concepts to numerical computation\n",
    "\n",
    "**Absolute error** is the magnitude of the error between an approximate result and the exact one $\\epsilon_{\\text{abs}} = |x_\\text{exact} - x_{\\text{approx}}|$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (1.0/(1.0/7.0**0.5)**3)**(2/3) #  exact answer is 7\n",
    "xexact = 7\n",
    "print(x)\n",
    "abserr = abs(x-xexact)\n",
    "print(abserr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While absolute error is very important, it requires that we have some understanding about how big an error is bad.  \n",
    "\n",
    "For instance, an error of a million would be a lot if counting the population of a town, but would likely be regarded as small if measuring the distance to the sun in meters (149,597,870,700m).\n",
    "\n",
    "**Relative error** can be defined as the ratio of the absolute error to the exact result \n",
    "\n",
    "$$ \\epsilon_{\\text{rel}} = |x_\\text{exact} - x_{\\text{approx}}| / |x_\\text{exact}| $$\n",
    "\n",
    "but other definitions are also useful depending on the circumstance, e.g.,\n",
    "\n",
    "$$ \\epsilon_{\\text{rel}} = |x_\\text{exact} - x_{\\text{approx}}| / \\text{max}(|x_\\text{exact}|,|x_\\text{approx}|) $$\n",
    "\n",
    "$$ \\epsilon_{\\text{rel}} = |x_\\text{exact} - x_{\\text{approx}}| / |x_\\text{approx}| $$\n",
    "\n",
    "From the perspective of relative error we can see the above computation was quite accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relerr = abs(x-xexact)/xexact\n",
    "print(relerr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Significant figures:** Relative error can also be interpreted as the number of significant figures or digits ($N$) in the value, where $N$ is computed as the largest integer such that \n",
    "\n",
    "$$ \\epsilon_{\\text{rel}} < 0.5 * 10^{-N} $$\n",
    "\n",
    "or \n",
    "\n",
    "$$ N \\approx \\text{floor}( -\\log_{10} (2* \\epsilon_{\\text{rel}}) ) $$\n",
    "\n",
    "(`floor` rounds towards zero --- i.e., takes a floating point value and discards the fraction).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{0:11d} {1}\".format(i[0],b(i[0])))\n",
    "from math import *\n",
    "sigfig = int(-log10(2*relerr))\n",
    "print(sigfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Precision** is often stated as the number of digits (or relative error) used to store or compute a result. Sometimes this might be stated as relative error, or the absolute error (e.g., if fixed-point arithmetic is being used instead of floating point).\n",
    "\n",
    "For example, here is an approximation to $\\pi$ (hint, it is not a particularly good approximation)\n",
    "```\n",
    "   piapprox = 3.1415243098283216\n",
    "```\n",
    "that has been specified with a precision of 17 digits.  We will see below that IEEE double-precision floating-point arithmetic has a precision of about 16 digits (and specifying a correctly rounded value can require a few more digits).\n",
    "\n",
    "Note: if we had specified more digits that they would have been discarded --- double-precision simply cannot hold any more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piapprox = 3.1415243098283216\n",
    "print(piapprox)\n",
    "piapprox = 3.1415243098283216217439821748972198\n",
    "print(piapprox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Representation error:** because of finite precision some values cannot be exactly represented --- this is usually not a problem but can lead to some unexpected outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(3.1415243098283217)  # focus on the last digit\n",
    "print(\"%.17f\" % 0.2)       # focus on the last digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "math.sin(math.pi)          # pi is not exactly representable, so sin(pi) cannot be exactly zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy** is often stated as the number of significant figures in a value comparing to the exact or true value. Sometimes relative or absolute accuracy might be used.\n",
    "\n",
    "Clearly the attainable accuracy is limited by the precision of computation, but there may be other limits\n",
    "*  finite accuracy of values input into a calculation\n",
    "*  finite accuracy of an algorithm to compute something\n",
    "\n",
    "Looking at the above approximate value for $\\pi$ that was stated with a precision of 17 digits we can see that it is only accurate to a little more than 4 decimal digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"approx:\", piapprox)\n",
    "print(\" exact:\", math.pi, \"(i.e., the closest floating-point representation of pi)\")\n",
    "pirelerr = abs(piapprox-math.pi)/math.pi\n",
    "print(\"relerr:\", pirelerr)\n",
    "N = int(-log10(2*pirelerr))\n",
    "print(\"sigfig:\", N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Floating point numbers \n",
    "\n",
    "Nearly all languages, including Python and NumPy, use the computer hardware supported IEEE 754 representation. \n",
    "* Note that libraries (e.g., MPFR) and packages (e.g., Maple or Mathematica) exist that provide greater precision at the price of loss of speed.  For Python, the module `mpmath` (http://mpmath.org/) provides extended precision arithmetic and we use this a bit further below.\n",
    "\n",
    "E.g., in 64-bit (https://en.wikipedia.org/wiki/Double-precision_floating-point_format)\n",
    "\n",
    "$$(-1)^s \\times 1.m \\times 2^{e-1023}$$\n",
    "\n",
    "- 1 bit for sign ($s$)\n",
    "- 11 bits for exponent ($e$)\n",
    "  - In range -1024 to +1023 in binary, or $\\pm$308 in decimal.  Thus, numbers larger than circa $10^{308}$ will overflow, and numbers smaller than circa $10^{-308}$ will underflow (gradually due to denormalized representation)\n",
    "- 52 bits for mantissa ($m$) giving 53 bits of significand \n",
    "  - 53 bits since we know for a non-zero number that the leading binary digit is 1 so we don't bother storing that\n",
    "  - $2^{-53} \\approx$ `1.11e-16`\n",
    "- Special values are reserved to represent\n",
    "  - Signed zero\n",
    "  - Overflow (number too in magnitude to represent) --- $\\pm \\infty$\n",
    "  - Not a number (result is not a valid number) --- `NaN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.23e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0.0000000000001234\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For native Python these limits and associated values can be found in `sys.float_info`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.float_info)\n",
    "print()\n",
    "print(\"maximum floating point number is\", sys.float_info.max)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Or for Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.finfo(float)) # or np.finfo(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrating what happens when you exceed the most positive exponent --- overflow\n",
    "x = 10.0**308\n",
    "print(x)\n",
    "x *= 2.0\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrating what happens when you exceed the most positive exponent and mantissa --- overflow\n",
    "import sys\n",
    "x = sys.float_info.max\n",
    "print(x)\n",
    "x = x*1.0000000000000002\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrating what happens as you approach and exceed the most negative exponent (i.e., very small numbers) \n",
    "# --- gradual underflow and loss of precision\n",
    "x = 1.23456789012345678e-300\n",
    "while x>0:\n",
    "    print(x)\n",
    "    x *= 0.1\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine epsilon** (https://en.wikipedia.org/wiki/Machine_epsilon) is the smallest positive number such that \n",
    "\n",
    "$1 + \\epsilon \\ne 1$\n",
    "\n",
    "It is the **relative error** for floating point computation.\n",
    "\n",
    "For any real value $x$ (assuming no over/underflow) there exists a numerical representation $x^\\prime$ such that $|x-x^\\prime| < \\epsilon |x|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing epsilon (normally would just look in sys.float_info)\n",
    "for n in range(-60,1):\n",
    "    epsilon = 2.0**n\n",
    "    print(n, epsilon, 1.0+epsilon)\n",
    "    if (1.0+epsilon) != 1.0:\n",
    "        break\n",
    "print()\n",
    "print(sys.float_info.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rounding error:** Is related to representation error already introduced above. While some numbers and floating computations can be, or can appear to be, exact, most suffer rounding error because the significand has only 53 bits, and the last bit must usually be rounded. \n",
    "\n",
    "This gives you 15-16 significant decimal figures.\n",
    "\n",
    "In IEEE 754 floating point arithmetic the default rounding mode is towards the closest exactly representable number.  Other rounding modes are available (e.g., round to zero, etc.) but are rarely needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "print(0.3/0.1 - 3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IEEE standard requires that the result of addition, subtraction, multiplication, division, square root, remainder, and conversion between integer and floating-point be *correctly rounded*.  It is not possible to do this efficiently for transcendental functions (e.g., `exp`) but these days most math libraries do correctly round all values and offer slightly less accurate modes (errors in the last 1-3 bits) that are potentially much faster.\n",
    "\n",
    "More precisely, let $\\times = +, -, *, /, \\ldots$ in exact arithmetic and let $\\otimes$ be the corresponding floating-point operation. Assuming no under/overflow, IEEE 754 arithmetic guarantees that given two floating-point values $x$ and $y$ that $|(x \\times y) - (x\\otimes y)| < \\epsilon |x \\times y|$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Floating point arithmetic is *commutative* but *not associative* and *not distributive*\n",
    "\n",
    "Commutative means $x \\otimes y = y\\otimes x$. \n",
    "* True in floating-point arithmetic for $\\otimes=+$ or $\\otimes=*$.\n",
    "\n",
    "Associative means $(x \\otimes y) \\otimes z = x \\otimes (y \\otimes z)$ where sub-expressions within parentheses are evaluated first. \n",
    "* Not true in floating point.\n",
    "\n",
    "Distributive means $x\\otimes(y + z) = x\\otimes y + x \\otimes z$ for $\\otimes = *, /$. \n",
    "* Not true in floating point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=239480912809.2930841092\n",
    "y=8309482109.193284092183018\n",
    "z=1.328488213048321094\n",
    "print(\"x*y == y*x\", x*y == y*x)  # * commutes\n",
    "print(\"x+y == y+x\", x+y == y+x)  # + commutes\n",
    "print(\"x-y == -(y-x)\", x-y == -(y-x)) # - commutes (taking care of sign)\n",
    "print(\"(x+y)+z == x+(y+z)\", (x+y)+z == x+(y+z)) # + is NOT exactly associative in floating point\n",
    "val1 = (x+y)+z\n",
    "val2 = x+(y+z)\n",
    "relerr = abs(val1-val2)/val1\n",
    "print(\"relative error in associative test\", relerr)\n",
    "print(\"x*(y+z)==x*y+x*z\", x*(y+z)==x*y+x*z) # * is NOT exactly distributive in floating point\n",
    "val1 = x*(y+z)\n",
    "val2 = x*y+x*z\n",
    "relerr = abs(val1-val2)/val1\n",
    "print(\"relative error in distributive test\", relerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reliably comparing floating-point values**\n",
    "\n",
    "You must pay attention when comparing two floating point numbers --- since floating computation is imprecise, comparing two numbers should be done allowing for some reasonable error.  But what is reasonable depends on what accuracy you are expecting --- i.e., often *you* have to decide what is acceptable, but a reasonable default can be obtained from the machine epsilon assuming just rounding error is present.\n",
    "\n",
    "The easiest way to do this is using `math.isclose` (https://docs.python.org/3/whatsnew/3.5.html#pep-485-a-function-for-testing-approximate-equality)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (math.pi+100.0)-100.0 # introduces a \"small\" error into the value of pi\n",
    "print(x==math.pi, x-math.pi)\n",
    "print(math.isclose(x, math.pi, rel_tol=1e-14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = math.pi\n",
    "y = x + 100000000000000\n",
    "print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y-100000000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pi + 1000000000000000 + (-1000000000000000)\n",
    "x = (math.pi + 1000000000000000) + (-1000000000000000)\n",
    "y = math.pi + (1000000000000000 + (-1000000000000000))\n",
    "print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cancellation error:** (loss of significance, https://en.wikipedia.org/wiki/Loss_of_significance) is a more pernicious problem.  \n",
    "\n",
    "Adding/subtracting numbers of similar magnitude to obtain a relatively small result or intermediate value can lose significant digits, sometimes catastrophically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0.1234567890123456\n",
    "y = 0.1234567890123000\n",
    "print(x)\n",
    "print(y)\n",
    "print(x-y)\n",
    "\n",
    "# Below we show how cancellation error can expose previous rounding error\n",
    "\n",
    "print((1.0+2.0**52)-2.0**52)   # Example of floating-point arithmetic not being distributive\n",
    "print((1.0+2.0**53)-2.0**53)\n",
    "print((math.pi+1e8)-1e8, math.pi) \n",
    "print((math.pi+1e16)-1e16, math.pi)\n",
    "print((math.pi + 1e17) - 1e17, math.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Summing data with varying magnitude and sign.\n",
    "\n",
    "Below we first define a function to make a random value that has a large variation in magnitude but is always positive.  Each value is exactly represented as a floating point number since each is just a small power of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "def ranval():\n",
    "    return 2.0**((random()-0.5)*100)\n",
    "\n",
    "# Print a few out to see what they look like\n",
    "for i in range(8):\n",
    "    print(ranval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sum a list of 1000 such values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values=[ranval() for i in range(1000)]\n",
    "print(\"sum in original order:   \", sum(values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Is there rounding error when computing the sum?\n",
    "\n",
    "**Question:** Is cancellation error a concern here? [Hint, the values are all positive.]\n",
    "\n",
    "**Question:** Should the order of summation greatly affect the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values.reverse()\n",
    "print(\"sum in reverse order:   \", sum(values))\n",
    "print(\"sum in sorted order:    \", sum(sorted(values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that any variation in the relative error is consistent with machine epsilon --- it is just rounding error.\n",
    "\n",
    "\n",
    "Next we make a list of 2000 elements --- the first 1000 are random values computed with the first function and the second 1000 are just the negative of the first.\n",
    "\n",
    "So we know the exact sum should be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values=[ranval() for i in range(1000)]\n",
    "values = values + [-value for value in values]\n",
    "print(\"sum in original order:   \", sum(values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Is cancellation error a concern here?\n",
    "\n",
    "**Question:** Why should the order of summation matter?\n",
    "\n",
    "**Question:** How can we get the computer to give us the \"correct\" answer?\n",
    "\n",
    "**Question:** In general, if we don't know the \"exact\" result, what do we even mean by \"correct?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sum in original order:  \", sum(values))\n",
    "values.reverse()\n",
    "print(\"sum in reverse order:   \", sum(values))\n",
    "print(\"sum in sorted order:    \", sum(sorted(values)))\n",
    "print(\"sum in abs sorted order:\", sum(sorted(values,key=abs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general even summing sorted data will still have some rounding error and may not even be the optimal algorithm --- we only get zero error here because of this special test case.\n",
    "\n",
    "If you are concerned about the accuracy of a sum of floating point values, you could try using the `math.fsum` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?math.fsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "math.fsum(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of the quadratic equation\n",
    "\n",
    "A classic example for numerical woes is the standard expression for the roots of a quadratic equation\n",
    "\n",
    "$ a x^2 + b x + c = 0$\n",
    "\n",
    "$ x = \\frac{-b \\pm \\sqrt{b^2 - 4 a c}}{2 a}$\n",
    "\n",
    "Three aspects of numerical computation conspire to make problems for this simple formula when $|ac| \\ll b^2$:\n",
    "*  Rounding error when computing $b^2 - 4 a c$\n",
    "*  Cancellation error when computing $-b \\pm \\sqrt{b^2 - 4 a c}$\n",
    "\n",
    "Consider adding two numbers $p+q$.  We've already seen that if $q$ is small compared to $p$ the addition operation must discard some of the digits in $q$. In the worst cast, if $|q|<\\epsilon |p|$ then $p+q=p$ in floating-point arithmetic (where $\\epsilon$ is the machine epsilon).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = np.linspace(-2,4,31)\n",
    "\n",
    "y = x**2 - 2*x - 3   # (x+1)*(x-3) is zero if x=-1 or x=+3\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(x,y)\n",
    "ax.axvline(x=-1, color='r', linestyle='dashed')\n",
    "ax.axvline(x= 3, color='r', linestyle='dashed')\n",
    "ax.axhline(y=0, color='r');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=1.0\n",
    "q=1.2345678901234567e-12\n",
    "print(p,q,p+q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coming back to the quadratic equation problem, imagine that $|ac| < \\epsilon b^2$ (where $\\epsilon$ is machine-epsilon).\n",
    "* The floating-point value for $b^2 - 4 a c$ will be computed as $b^2$ --- can you explain why?\n",
    "* As a result (and assuming that $b>0$) we will compute $-b + \\sqrt{b^2 - 4 a c}$ to have the value zero. \n",
    "  * This is the catastrophic cancellation error --- we have lost all information.\n",
    "* Again assuming $|ac| \\ll b^2$, a little bit of math (Taylor series) tells us that the correct answer is  $-b + \\sqrt{b^2 - 4 a c} \\approx + 2 ac/b$ and so that the corresponding root is $x \\approx c/b$\n",
    "* Similar problems arise for the other root if $b<0$.\n",
    "\n",
    "These errors can be avoided by using the alternative formula \n",
    "\n",
    "$$x = \\frac{2c}{-b \\mp \\sqrt{b^2 - 4 a c}}$$\n",
    "\n",
    "with '-' when $b\\ge 0$ and '+' when $b<0$.\n",
    "\n",
    "The original and alternative algorithms are implemented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def roots1(a, b, c):\n",
    "    r = math.sqrt(b**2 - 4 * a * c)\n",
    "    return (-b - r) / (2 * a), (-b + r) / (2 * a)\n",
    "\n",
    "\n",
    "def roots2(a, b, c):\n",
    "    r = math.sqrt(b**2 - 4 * a * c)\n",
    "    x1 = (-b - math.copysign(r, b)) / (2 * a)\n",
    "    x2 = c / (a * x1)\n",
    "    return (x1, x2)\n",
    "\n",
    "\n",
    "#\n",
    "print(\"roots1:\", roots1(1.0, 1e8, 1.0))\n",
    "print(\"roots2:\", roots2(1.0, 1e8, 1.0))\n",
    "print(\"exact:\", (-1e8, -1e-08))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What else can go wrong?**  \n",
    "* What if $a$ is zero?  I.e., you have a straight line instead of a parabola. The alternative formula works in this instance.\n",
    "* Can also get loss of signficance if $b^2 \\approx 4 a c$ but fixing this is not so easy unless we use extended precision.\n",
    "\n",
    "\n",
    "**Extended precision:** We can easily do this in native Python but not with NumPy.  In extended precision arithmetic we can use more bits in the mantissa and have arbitrarily large exponents --- but the price is a significant loss of speed.  Also, it is not a magical solution --- some algorithms can be so badly conditioned that it would be impossible to guarantee sufficient precision especially if there is only finite precision or accuracy in the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpmath as mp\n",
    "def roots3(a,b,c):\n",
    "    saveprec, mp.mp.prec = mp.mp.prec, 108 # set precision to 108 bits\n",
    "    a,b,c = mp.mpf(a),mp.mpf(b),mp.mpf(c)  # convert to quadruple precision\n",
    "    r = mp.sqrt(b**2 - 4*a*c)\n",
    "    if b < 0: r = -r\n",
    "    x1 = (-b - r)/(2*a)\n",
    "    x2 = c/(a*x1)\n",
    "    mp.mp.prec = saveprec                  # reset mp precision \n",
    "    return (float(x1),float(x2))           # return Python floats\n",
    "\n",
    "# roots should be\n",
    "x1 = 1.000000028975958\n",
    "x2 = 1.000000000000000\n",
    "print(\"roots1:\",roots1(94906265.625,-189812534.0,94906268.375))    \n",
    "print(\"roots2:\",roots2(94906265.625,-189812534.0,94906268.375))    \n",
    "print(\"roots3:\",roots3(94906265.625,-189812534.0,94906268.375))    \n",
    "print(\"eaxct: \",(x1,x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extended precision is implemented in software so it is slow compared to double precision that is directly implemented in hardware. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.mp.prec = 200 # 200 bits precision\n",
    "doubles = list(float(value) for value in range(2000))\n",
    "mpfs = [mp.mpf(value) for value in doubles]\n",
    "%timeit sum(doubles)\n",
    "%timeit sum(mpfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.array(doubles)\n",
    "%timeit np.sum(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
